library("faraway")
update.packages()
library("faraway")
install.packages("lme4")
library("faraway")
install.packages("farawaay")
install.packages("farawaay")
install.packages("faraway")
library("faraway")
library("faraway")
install.packages("faraway")
library("faraway")
?seatpos
dim(seatpos)
is.na(seatpos)
which(is.na(seatpos))
sum(is.na(seatpos))
# exploratory data analysis
corrplot(cor(seatpos), method= "number", type="upper", diag=FALSE)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac3.R", echo=TRUE)
cor(seatpos$hipcenter, seatpos$hipcenter)
cor(seatpos[,9], seatpos[,-9])
# it may also be helpful to look at a pairs plot
pairs(seatpos, pch=16, col=2)
# 3.2 - Ridge Regression
y = seatpos$hipcenter
x = seatpos[1,8]
x = seatpos[, !names(seatpos) %in% c("hipcenter")]
library(glmnet)
install.packages("glmnet")
library(glmnet)
?glmnet
# fit a ridge regression model
model = glmnet(x, y, nlambda=200)
View(model)
# fit a ridge regression model
model = glmnet(x, y, alpha=0, nlambda=200)
# fit a ridge regression model
ridge = glmnet(x, y, alpha=0, nlambda=200)
plot(ridge, xvar= 'lambda')
# 3.3 - Lasso Regression
lasso = glmnet(x, y)
View(lasso)
plot(lasso, xvar='lambda')
plot(lasso)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac3.R", echo=TRUE)
lasso$lamda
lasso$lambda
lasso$beta
prcomp
# 3.4 - Principal Component Analysis
seatpos.pr1 = prcomp(x, scale=TRUE)
seatpos.pr1
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac3.R", echo=TRUE)
seatpos.pr
prcomp(x, scale=TRUE)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac3.R", echo=TRUE)
barplot( PVE, names.arg = 1:9, main = "scree plot",
xlab = "number of PCs",
ylab = "proportion of variance explained" )
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac3.R", echo=TRUE)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac3.R", echo=TRUE)
# scree plot
plot(seatpos.pr)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac3.R", echo=TRUE)
summary(seatpos.pr)
library(MASS)
sum(is.na(Bostom))
sum(is.na(Boston))
names(Boston)
head(Boston)
head(cbind(Boston$medv, Boston$lstat))
?Boston
x.lab = 'Lower Status (%)'
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/r-stuff/polynomial_regression.R", echo=TRUE)
plot( x, y, cex.lab = 1.1, col="darkgrey", xlab = x.lab, ylab = y.lab,
main = "", bty = 'l' )
summary( poly2 )
poly2 = lm( y ~ x + I(x^2) )
summary( poly2 )
?I
# as this notation gets awkward, we can replace this using the 'poly' method
poly2 = lm(y ~ poly(x,  2,  raw = TRUE))
summary(poly2)
sort.x[1:10]     # the first 10 sorted values of x
sort.x = sort(x)
sort.x[1:10]     # the first 10 sorted values of x
pred2 = predict(poly2, newdata = list(x = sort.x), se = TRUE)
names(pred2)
pred2$fit[1:10]  # the first 10 fitted values of the curve
se.bands2 = cbind( pred2$fit - 2 * pred2$se.fit,
pred2$fit + 2 * pred2$se.fit )
se.bands2[1:10,] # the first 10 confidence intervals of the curve
plot(x, y, cex.lab = 1.1, col="darkgrey", xlab = x.lab, ylab = y.lab,
main = "Degree-2 polynomial", bty = 'l')
lines(sort.x, pred2$fit, lwd = 2, col = "red")
matlines(sort.x, se.bands2, lwd = 1.4, col = "red", lty = 3)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/r-stuff/polynomial_regression.R", echo=TRUE)
poly1 = lm(y ~ x)
poly6 = lm(y ~ poly(x, 6))
anova(poly1, poly2, poly3, poly4, poly5, poly6)
x1=Boston$lstat
x2=Boston$rm
polym1 <- lm(y ~ poly(x1, 2) + poly(x2 , 2) + x1:x2)
summary(polym1)
# we could use polym() makes things easier for us
polym2=lm(y ~ polym(x1, x2, degree=2) )
summary(polym2)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/r-stuff/polynomial_regression.R", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
table(cut(x, 2))
step2 = lm(y ~ cut(x, 2))
step3 = lm(y ~ cut(x, 3))
step4 = lm(y ~ cut(x, 4))
step5 = lm(y ~ cut(x, 5))
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/r-stuff/step_function.R", echo=TRUE)
library("faraway")
library("MASS")
library("faraway")
y = seatpos$hipcenter
x = seatpos$HT
x = seatpos$Ht
plot(x, y)
?ploy
?plot
?seatpos
plot(x, y, xlab="Hip Center", ylab="Bare foot height (cm)")
# polynomial regression and step functions
lin1 = lm(y~x) # first order
poly2 = lm(y~x + I(x^2)) # 2nd order
summary(lin1)
summary(poly2)
pred1 = predict(lin1, newdata=list(x = sort.x), se=TRUE)
# plotting confidence intervals
sort.x = sort(x) # sorted values of x
pred1 = predict(lin1, newdata=list(x = sort.x), se=TRUE)
se.bands2 = cbind(pred1$fit - 2 * pred1$se.fit,
pred1$fit + 2 * pred1$se.fit)
matlines(sort.x, se.bands1, lwd = 1.4, col = "red", lty = 3)
plot(x, y, cex.lab = 1.1, col="darkgrey", xlab = x.lab, ylab = y.lab,
main = "Degree-2 polynomial", bty = 'l')
lines(sort.x, pred1$fit, lwd = 2, col = "red")
matlines(sort.x, se.bands1, lwd = 1.4, col = "red", lty = 3)
se.bands1 = cbind(pred1$fit - 2 * pred1$se.fit,
pred1$fit + 2 * pred1$se.fit)
plot(x, y, cex.lab = 1.1, col="darkgrey", xlab = x.lab, ylab = y.lab,
main = "Degree-2 polynomial", bty = 'l')
lines(sort.x, pred1$fit, lwd = 2, col = "red")
matlines(sort.x, se.bands1, lwd = 1.4, col = "red", lty = 3)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac4.R", echo=TRUE)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac4.R", echo=TRUE)
# poly1
plot(x, y, cex.lab = 1.1, col="darkgrey", xlab = x.lab, ylab = y.lab,
main = "Degree-1 polynomial", bty = 'l')
lines(sort.x, pred1$fit, lwd = 2, col = "red")
matlines(sort.x, se.bands1, lwd = 1.4, col = "red", lty = 3)
# poly2
plot(x, y, cex.lab = 1.1, col="darkgrey", xlab = x.lab, ylab = y.lab,
main = "Degree-2 polynomial", bty = 'l')
lines(sort.x, pred2$fit, lwd = 2, col = "blue")
matlines(sort.x, se.bands2, lwd = 1.4, col = "blue", lty = 3)
matlines(sort(x), se.bands6, lwd = 1.4, col = "red", lty = 3)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac4.R", echo=TRUE)
# step function
step6 = lm(y ~ cut(x, y))
# step function
step6 = lm(y ~ cut(x, y))
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac4.R", echo=TRUE)
linear_spline = lm(y~bs(x, degree = 1, knots=cuts))
library("ISLR")
library("splines")
linear_spline = lm(y~bs(x, degree = 1, knots=cuts))
# ex 4.2 - splines
cuts = summary(x)[c(2,3,5)] # get percentiles
linear_spline = lm(y~bs(x, degree = 1, knots=cuts))
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac4.R", echo=TRUE)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac4.R", echo=TRUE)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac4.R", echo=TRUE)
# ex 4.2 - splines
cuts = summary(x)[c(2,3,5)] # get percentiles
spline1 = lm(y~bs(x, degree = 1, knots=cuts))
sort.x = sort(x)
pred3 = predict(spline1, newdata=sort.x, se=TRUE)
pred3 = predict(spline1, newdata=list(sort.x), se=TRUE)
se.spline1 = cbind(pred3$fit + 2*pred3$se.fit, pred3$fit-2*pred3$se.fit)
# plot spline
plot(x, y, cex.lab = 1.1, col="darkgrey", xlab = x.lab, ylab = y.lab,
main = "Linear Spline", bty = 'l')
lines(sort.x, pred3$fit, lwd = 2, col = "red")
matlines(sort.x, se.spline1, lwd = 1.4, col = "red", lty = 3)
par(mfrow=c(1,1))
# ex 4.2 - splines
cuts = summary(x)[c(2,3,5)] # get percentiles
spline1 = lm(y~bs(x, degree = 1, knots=cuts))
sort.x = sort(x)
pred3 = predict(spline1, newdata=list(sort.x), se=TRUE)
se.spline1 = cbind(pred3$fit + 2*pred3$se.fit, pred3$fit-2*pred3$se.fit)
# plot spline
plot(x, y, cex.lab = 1.1, col="darkgrey", xlab = x.lab, ylab = y.lab,
main = "Linear Spline", bty = 'l')
lines(sort.x, pred3$fit, lwd = 2, col = "red")
matlines(sort.x, se.spline1, lwd = 1.4, col = "red", lty = 3)
smooth1 = smooth.spline(x, y, cv=TRUE) # select lambda via cross-validation
plot(x, y, cex.lab = 1.1, col="darkgrey", xlab = x.lab, ylab = y.lab,
main = "Smoothing Spline Model", bty = 'l')
smooth1 = smooth.spline(x, y, df=3) # select lambda via cross-validation
plot(x, y, cex.lab = 1.1, col="darkgrey", xlab = x.lab, ylab = y.lab,
main = "Smoothing Spline Model", bty = 'l')
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac4.R", echo=TRUE)
library(gam)
install.packages("gam")
library("gam")
library("gam")
?gam
?ns
gam1 = gam(hipcenter~ns(Age, df=5) + smooth.spline(Age, hipcenter, df=3) + lm(hipcenter~Ht), data=seatpos)
gam1 = gam(hipcenter~ns(Age, df=5) + s(Age, df=3) + Ht,
data=seatpos)
par( mfrow = c(1,3) )
plot( gam,  se = TRUE, col = "blue" )
par( mfrow = c(1,3) )
plot( gam1,  se = TRUE, col = "blue" )
summary(gam1)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac4.R", echo=TRUE)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac4.R", echo=TRUE)
y = c(1,3,6,25,73,221,294,257,236,189,125,67,26,10,3)
library(ggplot2)
1:15
c(1:15)
df = data.frame(date = c(0:14))
df = data.frame(date = c(0:14), y)
View(df)
source("C:/Users/guyro/OneDrive/Y3/bayesian-modelling/prac3.R", echo=TRUE)
p
source("C:/Users/guyro/OneDrive/Y3/bayesian-modelling/prac3.R", echo=TRUE)
log(rnorm(3))
rnorm(3)
rlnorm(3)
sum(dnorm(0.5, 0.5, 0, log=TRUE))
library(MASS)
MH(1000, df, 0.1, Sigma=diag(0.001,0.001,0.001), init=c(-6,log(0.5),2.5), x0=(762,1))
MH(1000, df, 0.1, Sigma=diag(0.001,0.001,0.001), init=c(-6,log(0.5),2.5), x0=c(762,1))
MH=function(N,data,dt,Sigma,init,x0)
{
theta.mat = matrix(0,nrow=N,ncol=3)
theta.mat[1,] = init
curr = init
count = 0
for (j in 2:N)
{
can = mvrnorm(1,curr,Sigma) #proposal
#Evaluate log acceptance probability
laprob = logprior(theta.mat[j,])
if (log(runif(1)) < laprob) #Accept with the correct prob.
{
curr = can; #chain moves
count = count + 1 #add one to acceptance counter
}
theta.mat[j,] = curr
}
print(count/(N-1)) # empirical acceptance prob
return(theta.mat)
}
MH(1000, df, 0.1, Sigma=diag(0.001,0.001,0.001), init=c(-6,log(0.5),2.5), x0=c(762,1))
MH(N=1000, data=df, dt=0.1, Sigma=diag(0.001,0.001,0.001), init=c(-6,log(0.5),2.5), x0=c(762,1))
diag(0.001,0.001,0.001)
diag(1)
diag(2)
diag(x=2, value=0.001)
diag(0.001, 3, 3)
MH(N=1000, data=df, dt=0.1, Sigma=diag(0.001,3,3), init=c(-6,log(0.5),2.5), x0=c(762,1))
logprior = function(theta) {
return(sum(dnorm(theta, log=TRUE)))
}
MH=function(N,data,dt,Sigma,init,x0)
{
theta.mat = matrix(0,nrow=N,ncol=3)
theta.mat[1,] = init
curr = init
count = 0
for (j in 2:N)
{
can = mvrnorm(1,curr,Sigma) #proposal
#Evaluate log acceptance probability
laprob = logprior(theta.mat[j,])
if (log(runif(1)) < laprob) #Accept with the correct prob.
{
curr = can; #chain moves
count = count + 1 #add one to acceptance counter
}
theta.mat[j,] = curr
}
print(count/(N-1)) # empirical acceptance prob
return(theta.mat)
}
MH(N=1000, data=df, dt=0.1, Sigma=diag(0.001,3,3), init=c(-6,log(0.5),2.5), x0=c(762,1))
hist(out)
out = MH(N=1000, data=df, dt=0.1, Sigma=diag(0.001,3,3), init=c(-6,log(0.5),2.5), x0=c(762,1))
hist(out)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/r-stuff/mock_practical_exam.R", echo=TRUE)
install.packages("ISLR")
# Accuracy
tree.pred = predict(tree.carseats, data_test, type="class")
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/r-stuff/mock_practical_exam.R", echo=TRUE)
install.packages("tree")
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/r-stuff/mock_practical_exam.R", echo=TRUE)
install.packages("modeldata")
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/r-stuff/mock_practical_exam.R", echo=TRUE)
install.packages("randomForest")
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/r-stuff/mock_practical_exam.R", echo=TRUE)
install.packages("gbm")
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/r-stuff/mock_practical_exam.R", echo=TRUE)
table(tree.pred, data_test$High)
# Accuracy
tree.pred = predict(tree.carseats, data_test, type="class")
table(tree.pred, data_test$High)
# Accuracy
tree.pred = predict(tree.carseats, data_test, type="class")
library(ISLR)
library(tree)
attach(Carseats)
High = ifelse(Sales<=8, "No", "Yes")
Carseats = data.frame(Carseats, High)
Carseats$High = as.factor(Carseats$High)
tree.carseats = tree(High~.-Sales, Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0)
set.seed(743)
train_index=sample(1:nrow(Carseats), 250)
data_train = Carseats[train_index,]
data_test = Carseats[-train_index,]
tree.carseats = tree(High~.-Sales, data_train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty =0)
# Accuracy
tree.pred = predict(tree.carseats, data_test, type="class")
table(tree.pred, data_test$High)
yhat = predict(tree.carseats, data_test, type="class")
summary(yhat)
summary(data_test$High) # we get 15 false responses
install.packages("leaps")
# 2.6
library(leaps)
fwd = regsubsets(brozek~., data=fat1)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac2.R", echo=TRUE)
summary(fws)
summary(fwd)
names(fwd)
fwd$rss
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac1_term2.R", echo=TRUE)
# note: z is the dataset
gradient_descent <- function(w, z, t, lr, n) {
# create the chain for w
w_chain = c()
for (i in 1:t) {
# update the values
erf_fun <- function(w, z = z_obs, n=n_obs) {
return( empirical_risk_fun(w, z, n) )
}
# use an adaptive learning rate...
lr = t0/t
# update w (grad. descent alg)
w = w - lr*numDeriv::grad(erf_fun, w)
# store the values in a vector (for later!)
w_chain = rbind(w_chain, w)
}
return (w_chain)
}
w_vals = gradient_descent(c(-0.3, 3), z_obs, 100, 0.1, n_obs)
# Task 1.1
t0 = 1
# 1.3
w_vals1 = gradient_descent(c(-0.3, 3), z_obs, 1000, 0.99, n_obs)
print(w_vals1[25])
# Task 1.1
t0 = 10
# note: z is the dataset
gradient_descent <- function(w, z, t, lr, n) {
# create the chain for w
w_chain = c()
for (i in 1:t) {
# update the values
erf_fun <- function(w, z = z_obs, n=n_obs) {
return( empirical_risk_fun(w, z, n) )
}
# use an adaptive learning rate...
# lr = t0/t
lr = learning_rate(t, t0)
# update w (grad. descent alg)
w = w - lr*numDeriv::grad(erf_fun, w)
# store the values in a vector (for later!)
w_chain = rbind(w_chain, w)
}
return (w_chain)
}
w_vals = gradient_descent(c(-0.3, 3), z_obs, 100, 0.1, n_obs)
plot(w_vals[, 1])
plot(w_vals[, 2])
# 1.3
w_vals1 = gradient_descent(c(-0.3, 3), z_obs, 100, 0.99, n_obs)
w_vals1
# we can see that our plots converge much faster here
plot(w_vals1[,1])
print(w_vals1[25])
# Task 1.1
t0 = 10
# note: z is the dataset
gradient_descent <- function(w, z, t, lr, n) {
# create the chain for w
w_chain = c()
for (i in 1:t) {
# update the values
erf_fun <- function(w, z = z_obs, n=n_obs) {
return( empirical_risk_fun(w, z, n) )
}
# use an adaptive learning rate...
# lr = t0/t
lr = learning_rate(i, t0)
# update w (grad. descent alg)
w = w - lr*numDeriv::grad(erf_fun, w)
# store the values in a vector (for later!)
w_chain = rbind(w_chain, w)
}
return (w_chain)
}
w_vals = gradient_descent(c(-0.3, 3), z_obs, 100, 0.1, n_obs)
plot(w_vals[, 1])
plot(w_vals[, 2])
# 1.3
w_vals1 = gradient_descent(c(-0.3, 3), z_obs, 100, 0.99, n_obs)
w_vals1
# we can see that our plots converge much faster here
plot(w_vals1[,1])
print(w_vals1[25])
View(model)
gc()
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac1_term2.R", echo=TRUE)
# 1.3
w_vals1 = gradient_descent(c(-0.3, 3), z_obs, 100, 0.99, n_obs, 1)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac1_term2.R", echo=TRUE)
plot(wvals2[, 1])
wvals2 = batch_gradient_descent(c(-0.3, 3), z_obs, 100, 0.01, 10, n_obs)
plot(wvals2[, 1])
wvals2 = batch_gradient_descent(c(-0.3, 3), z_obs, 1000, 0.01, 10, n_obs)
plot(wvals2[, 1])
plot(wvals2[, 2])
plot(wvals2[, 1])
wvals2 = batch_gradient_descent(c(-0.3, 3), z_obs, 1000, 0.2, 10, n_obs)
plot(wvals2[, 1])
wvals2 = batch_gradient_descent(c(-0.3, 3), z_obs, 1000, 0.001, 10, n_obs)
plot(wvals2[, 1])
wvals2 = batch_gradient_descent(c(-0.3, 3), z_obs, 1000, 0.0005, 10, n_obs)
plot(wvals2[, 1])
wvals2 = batch_gradient_descent(c(-0.3, 3), z_obs, 100, 0.001, 10, n_obs)
plot(wvals2[, 1])
wvals2 = batch_gradient_descent(c(-0.3, 3), z_obs, 10000, 0.001, 50, n_obs)
plot(wvals2[, 1])
set.seed(2023)
n_obs <- 1000000
w_true <- c(0,1)
z_obs <- data_generating_model(n = n_obs, w = w_true)
w_true <- as.numeric(glm(z_obs[,2]~ 1+ z_obs[,1],family = "binomial" )$coefficients)
source("C:/Users/guyro/OneDrive/Y3/Machine_Learning_and_Neural_Networks_III_Epiphany_2023/Lecture_handouts/code/04.Stochastic_gradient_Langevine_dynamics/example_SGLD.R", echo=TRUE)
setwd
?setwd
getwd
getwd()
setwd('C:\Users\guyro\OneDrive\Y3\Machine_Learning_and_Neural_Networks_III_Epiphany_2023\Lecture_handouts\code\04.Stochastic_gradient_Langevine_dynamics')
cls
remove.packages("rstan")
if (file.exists(".RData")) file.remove(".RData")
Sys.setenv(DOWNLOAD_STATIC_LIBV8 = 1) # only necessary for Linux without the nodejs library / headers
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)
example(stan_model, package = "rstan", run.dontrun = TRUE)
# Compile packages using all cores
Sys.setenv(MAKEFLAGS = paste0("-j",parallel::detectCores()))
install.packages(c("StanHeaders","rstan"),type="source")
example(stan_model, package = "rstan", run.dontrun = TRUE)
library("rstan")
source("C:/Users/guyro/OneDrive/Y3/bayesian-modelling/stan/schools.R", echo=TRUE)
source("C:/Users/guyro/OneDrive/Y3/bayesian-modelling/stan/schools.R", echo=TRUE)
getwd()
setwd("C:/Users/guyro/OneDrive/Y3/bayesian-modelling/stan")
getwd()
source("C:/Users/guyro/OneDrive/Y3/bayesian-modelling/stan/schools.R", echo=TRUE)
source("C:/Users/guyro/OneDrive/Y3/bayesian-modelling/stan/schools.R", echo=TRUE)
remove.packages("rstan")
if (file.exists(".RData")) file.remove(".RData")
install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
R.version
remove.packages("rstan")
if (file.exists(".RData")) file.remove(".RData")
Sys.setenv(DOWNLOAD_STATIC_LIBV8 = 1) # only necessary for Linux without the nodejs library / headers
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)
example(stan_model, package = "rstan", run.dontrun = TRUE)
remove.packages("rstan")
if (file.exists(".RData")) file.remove(".RData")
install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
remove.packages("rstan")
if (file.exists(".RData")) file.remove(".RData")
Sys.setenv(DOWNLOAD_STATIC_LIBV8 = 1) # only necessary for Linux without the nodejs library / headers
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)
example(stan_model, package = "rstan", run.dontrun = TRUE)
# Compile packages using all cores
Sys.setenv(MAKEFLAGS = paste0("-j",parallel::detectCores()))
install.packages(c("StanHeaders","rstan"),type="source")
