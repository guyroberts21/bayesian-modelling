}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * numDeriv::grad(f, w)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
batch_gradient_descent(c(-0.3, 3), z_obs, 1000, 0.01, 10, n_obs)
z_obs[x,]
View(z_obs)
install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write=TRUE)
X = rnorm(1000, mean=5, sd=1)
my_data = list(N=1000, X=X)
./
exit()
getwd()
fit = stan(file='C:/Users/guyro/Desktop/DU/Maths/Y3/bayesian-modelling/my_model.stan', data=my_data)
example(stan_model, package = 'rstan', run.dontrun = TRUE)
remove.packages("rstan")
if (file.exists(".RData")) file.remove(".RData")
Sys.setenv(DOWNLOAD_STATIC_LIBV8 = 1) # only necessary for Linux without the nodejs library / headers
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)
example(stan_model, package = "rstan", run.dontrun = TRUE)
library(rstan)
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)
example(stan_model, package = "rstan", run.dontrun = TRUE)
library(rstan)
# Compile packages using all cores
Sys.setenv(MAKEFLAGS = paste0("-j",parallel::detectCores()))
install.packages(c("StanHeaders","rstan"),type="source")
library(rstan)
library(rstan)
stan
?stan()
install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
example(stan_model, package = "rstan", run.dontrun = TRUE)
library(numDeriv)
library(mvtnorm)
set.seed(2023)
data_generating_model <- function(n,w) {
d <- 3
z <- rep( NaN, times=n*d )
z <- matrix(z, nrow = n, ncol = d)
z[,1] <- 1.0
z[,2] <- runif(n, min = -10, max = 10)
p <- w[1]*z[,1] + w[2]*z[,2]
p <- exp(p) / (1+exp(p))
z[,3] <- rbinom(n, size = 1, prob = p)
return(z)
}
n_obs <- 10^(6)
w_true <- c(0,1)
set.seed(2023)
z_obs <- data_generating_model(n = n_obs, w = w_true)
set.seed(0)
w_true <- as.numeric(glm(z_obs[,3]~ 1 + z_obs[,2],family = "binomial" )$coefficients)
prediction_rule <- function(x,w) {
h <- w[1]*x[1]+w[2]*x[2]
h <- exp(h) / (1.0 + exp(h) )
return (h)
}
log_sampling_pdf <- function(z, w) {
d <- length(w)
x <- z[1:d]
y <- z[d+1]
log_pdf <- y * log(prediction_rule(x,w)) +(1-y) * log( 1.0-prediction_rule(x,w) )
#log_pdf <- dbinom(y, size = 1, prob = prediction_rule(x,w), log = TRUE)
return( log_pdf )
}
log_prior_pdf <- function(w, mu, Sig2 ) {
log_pdf <- dmvnorm(w, mean = mu, sigma = Sig2, log = TRUE, checkSymmetry = TRUE)
return( log_pdf )
}
learning_rate <- function(t, T_0 = 100, T_1 = 500, C_0 = 0.0001, s_0 = 0.5 ) {
if ( t <= T_0 ) {
eta <- C_0
} else if ( (T_0+1 <= t) && (t <= T_1 ) ) {
eta <- C_0 / ( (t-T_0) ^ s_0 )
} else {
eta <- C_0 / ( (T_1-T_0) ^ s_0 )
}
return(eta)
}
w = c(-0.1, 1.5)
ext_func = function(w, z=z_obs[1, ]) {
return(log_sampling_pdf(z, w))
}
numDeriv::grad(ext_func, w)
# Method for cacluating the gradient of log pdf without the library function
grad_log_sampling_pdf <- function(w,z) {
d <- length(w)
x = z[1:d]
y = z[d+1]
h <- prediction_rule(x,w)
grd <- -(h-y)*x
return (grd)
}
gr <- grad_log_sampling_pdf(w,z = z_obs[1,])
gr
online_sgd = function(w, data, num_iters, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = 1, replace = TRUE)
zbatch <- matrix(data[J,],1)
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * numDeriv::grad(ext_func, w)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
online_sgd(w, z_obs, 1000, n_obs)
batch_sgd = function(w, data, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = TRUE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * numDeriv::grad(ext_func, w)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
batch_sgd(w, z_obs, 3000, 16, n_obs)
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = FALSE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w + lrate * ((n_obs/batch_size) * numDeriv::grad(f, w) +
log_prior_pdf(w=w, mu=rep(0, 2), Sig2=100*diag(2))) +
sqrt(lrate)*sqrt(tau)*rnorm(2)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
wvals = batch_sgd_prior(w, z_obs, 1.0, 3000, 1000, n_obs)
plot(wvals[, 1])
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
lrate = learning_rate(t, 0.3*num_iters, 0.6*num_iters, 1e-3, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = FALSE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w + lrate * ((n_obs/batch_size) * numDeriv::grad(f, w) +
log_prior_pdf(w=w, mu=rep(0, 2), Sig2=100*diag(2))) +
sqrt(lrate)*sqrt(tau)*rnorm(2)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
w_seed = c(-1, 0)
w_chain = batch_sgd_prior(w_seed, z_obs, 1.0, 1000, 1000, n_obs)
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
1e-3
10^(-1)
10^(-2)
1e-2
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
lrate = learning_rate(t, 0.3*num_iters, 0.6*num_iters, 1e-2, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = FALSE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w + lrate * ((n_obs/batch_size) * numDeriv::grad(f, w) +
log_prior_pdf(w=w, mu=rep(0, 2), Sig2=100*diag(2))) +
sqrt(lrate)*sqrt(tau)*rnorm(2)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
w_seed = c(-1, 0)
w_chain = batch_sgd_prior(w_seed, z_obs, 1.0, 1000, 1000, n_obs)
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
w_seed = c(-1, 0)
w_chain = batch_sgd_prior(w_seed, z_obs, 1.0, 1000, 1000, n_obs)
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs, clipping_threshold) {
w_chain = c(w)
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
lrate = learning_rate(t, 0.3*num_iters, 0.6*num_iters, 1e-2, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = FALSE)
# 2. Calculate the likelihood (over all samples)
grad_est_lik = rep(0.0, times=length(w))
for (j in J) {
f = function(w, z=z_obs[j,]) {
return(log_sampling_pdf(z, w))
}
grad_est_lik = grad_est_lik + numDeriv::grad(f, w)
}
grad_est_lik = (n_obs / batch_size) * grad_est_lik
# 3. Apply clipping (in case gradients explode!)
norm_grad_est_lik = sqrt(sum(grad_est_lik^2))
grad_est_lik = grad_est_lik * min(1.0, clipping_threshold/norm_grad_est_lik)
w = w + lrate*grad_est_lik
# 4. Add the prior pdf
f = function(w, z=zbatch) {
d = length(w)
return(log_prior_pdf(w=w, mu=rep(0, 2), Sig2=100*diag(d)))
}
w = w + lrate*numDeriv::grad(f, w)
# 5. Add the noise
w = w + sqrt(lrate)*sqrt(tau)*rnorm(length(w))
# 6. Store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
w_seed = c(-1, 0)
w_chain = batch_sgd_prior(w_seed, z_obs, 1.0, 1000, 1000, n_obs)
w_chain = batch_sgd_prior(w_seed, z_obs, 1.0, 1000, 1000, n_obs, 10)
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
plot(w_chain[,2], type='l') +
abline(h=w_true[2], col='red')
set.seed(2023)
rnorm(2)
rnorm(2, 0, 1)
rnorm(2, 0, 1)
rnorm(2, 0, 1)
rnorm(2, 0, 1)
set.seed(2023)
rnorm(2, 0, 1)
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs, clipping_threshold) {
w_chain = c(w)
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
lrate = learning_rate(t, 0.3*num_iters, 0.6*num_iters, 1e-2, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = FALSE)
# 2. Calculate the likelihood (over all samples)
grad_est_lik = rep(0.0, times=length(w))
for (j in J) {
f = function(w, z=z_obs[j,]) {
return(log_sampling_pdf(z, w))
}
grad_est_lik = grad_est_lik + numDeriv::grad(f, w)
}
grad_est_lik = (n_obs / batch_size) * grad_est_lik
# 3. Apply clipping (in case gradients explode!)
norm_grad_est_lik = sqrt(sum(grad_est_lik^2))
grad_est_lik = grad_est_lik * min(1.0, clipping_threshold/norm_grad_est_lik)
w = w + lrate*grad_est_lik
# 4. Add the prior pdf
f = function(w, z=zbatch) {
d = length(w)
return(log_prior_pdf(w=w, mu=rep(0, d), Sig2=100*diag(d)))
}
w = w + lrate*numDeriv::grad(f, w)
# 5. Add the noise
w = w + sqrt(lrate)*sqrt(tau)*rnorm(length(w))
# 6. Store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
set.seed(2023)
w_seed = c(-1, 0)
w_chain = batch_sgd_prior(w_seed, z_obs, 1.0, 1000, 1000, n_obs, 10)
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
plot(w_chain[,2], type='l') +
abline(h=w_true[2], col='red')
library(nnet)
library("faraway")
install.packages("faraway")
library("faraway")
data(ozone)
?ozone
force(ozone)
View(ozone)
nn.out.1 = nnet(O3 ~ temp + ibh + ibt, ozone, size=2, linout=T)
?nnet
# linout True sets linear output units (which we want!)
# size 2 (2 layers)
# and then we define the formula as a linear combination (since)
nn.out.1 = nnet(O3 ~ temp + ibh + ibt, ozone, size=2, linout=T)
compound_dist = function(N, alpha) {
return(beta(alpha+1, N+1)/(beta(alpha, 1)))
}
compound_dist(10, 15)
compound_dist(10, 20)
compound_dist(10, 50)
compound_dist(10, 500)
compound_dist(10, 11)
compound_dist(2, 11)
compound_dist(2, 5)
compound_dist(2, 3)
compound_dist(2, 2)
compound_dist(2, 1)
compound_dist(2, 0)
compound_dist(2, 1)
compound_dist(2, 4)
compound_dist(2, 6)
compound_dist(2, 8)
compound_dist(2, 100)
compound_dist(2, 10000)
compound_dist(2, 1)
# manual test
(factorial(N + alpha) / (factorial(N)*factorial(alpha))) * (N+1)/(alpha+N+1)
N = 2
alpha = 1
compound_dist(N, alpha)
# manual test
(factorial(N + alpha) / (factorial(N)*factorial(alpha))) * (N+1)/(alpha+N+1)
compound_dist = function(N, alpha) {
pmf_N = choose(N + alpha, N) * beta(alpha+1, N+1)/beta(alpha, 1)
return(pmf_N)
}
N = 2
alpha = 1
compound_dist(N, alpha)
# manual test
(factorial(N + alpha) / (factorial(N)*factorial(alpha))) * (N+1)/(alpha+N+1)
alpha = 3
compound_dist(N, alpha)
# manual test
(factorial(N + alpha) / (factorial(N)*factorial(alpha))) * (N+1)/(alpha+N+1)
beta(2, 3)
beta(3, 2)
beta(100, 2)
beta(2, 100)
(gamma(2)*gamma(100))/gamma(102)
compound_dist = function(N, alpha) {
pmf_N = beta(alpha+1, N+1)/beta(alpha, 1) * factorial(N+alpha)
return(pmf_N)
}
N = 2
alpha = 3
compound_dist(N, alpha)
compound_dist = function(N, alpha) {
pmf_N = (beta(N+1, alpha+1)/beta(alpha, 1)) * factorial(N+alpha)
return(pmf_N)
}
N = 2
alpha = 3
compound_dist(N, alpha)
# manual test
(factorial(N + alpha) / (factorial(N)*factorial(alpha))) * (N+1)/(alpha+N+1)
compound_dist = function(N, alpha) {
pmf_N = beta(alpha+1, N+1)/(beta(alpha, 1) * factorial(N+alpha))
return(pmf_N)
}
N = 2
alpha = 3
compound_dist(N, alpha)
N = 2
alpha = 10
compound_dist(N, alpha)
alpha = 1
compound_dist(N, alpha)
beta(3,2)
compound_dist = function(N, alpha) {
pmf_N = pbeta(alpha+1, N+1)/pbeta(alpha, 1)
return(pmf_N)
}
N = 2
alpha = 1
compound_dist(N, alpha)
pmf_N = dbeta(alpha+1, N+1)/pbeta(alpha, 1)
pmf_N = beta(alpha+1, N+1)/pbeta(alpha, 1)
pmf_N = dbeta(alpha+1, N+1)/dbeta(alpha, 1)
compound_dist = function(N, alpha) {
numerator = beta(N+1, alpha+1)
denominator = beta(alpha, 1) * factorial(N + alpha)
return(numerator/denominator)
}
N = 2
alpha = 1
compound_dist(N, alpha)
compound_dist = function(N, alpha) {
numerator = beta(N+1, alpha+1)
denominator = beta(alpha, 1) * factorial(N + alpha)
return(numerator/denominator)
}
compound_dist(2, 1)
compound_dist = function(N, alpha) {
numerator = beta(N+1, alpha+1)
denominator = beta(alpha, 1) * factorial(N + alpha)
return(numerator/denominator)
}
compound_dist(2, 1)
View(compound_dist)
compound_dist <- function(N, alpha) {
numerator <- beta(N+1, alpha+1)
denominator <- beta(alpha, 1) * factorial(N + alpha)
return(numerator/denominator)
}
compound_dist(2, 1)
compound_dist <- function(N, alpha) {
numerator <- stats::beta(N+1, alpha+1)
denominator <- stats::beta(alpha, 1) * factorial(N + alpha)
return(numerator/denominator)
}
compound_dist(2, 1)
rstan:::rstudio_stanc("C:/Users/guyro/Desktop/DU/Maths/Y3/bayesian-modelling/lecture-stuff/chap10/model1.stan")
rstan:::rstudio_stanc("C:/Users/guyro/Desktop/DU/Maths/Y3/bayesian-modelling/lecture-stuff/chap10/model1.stan")
setwd('C:/Users/guyro/Desktop/DU/Maths/Y3/bayesian-modelling/assignments/assignment6')
setwd('C:/Users/guyro/Desktop/DU/Maths/Y3/bayesian-modelling/lecture-stuff/chap10/')
model = stan_model('model1.stan')
library(rstan)
setwd('C:/Users/guyro/Desktop/DU/Maths/Y3/bayesian-modelling/lecture-stuff/chap10/')
model = stan_model('model1.stan')
fit <- sampling(model,
data = list(N = 12,
x1 = c(5.3,5.1,4.8,4.5,
5.5,5.2,5.0,5.0,
5.1,4.6,4.3,5.3)),
iter = 10000)
samples = extract(fit$x1)
samples = extract(fit$[x1])
samples = extract(fit)$'x1'
samples
samples = extract(fit)$'x2'
samples
hist(samples)
plot(samples)
density = extract(fit$)x1
density = extract(fit)x1
density = extract(fit)$'x1'
x = extract(fit)$'tau'
x = extract(fit)$'omega'
x = extract(fit)$'mu[1]'
x = extract(fit)$'mu'
x = extract(fit)$'x'
x = extract(fit)$'mu'
x = extract(fit$)$'lp__'
x = extract(fit)$'lp__'
