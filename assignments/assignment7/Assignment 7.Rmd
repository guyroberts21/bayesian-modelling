---
title: "Solutions for assignment 7"
author: "Guy Roberts"
date: "2023-03-04"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Solution to 1.1

In order to answer this question, we used trial and error of varying values of $\alpha$, this is because we know that:

\begin{align*}
    Pr(N \leq 2) &= \pi(2) + \pi(1) + \pi(0)
\end{align*}

We find that our solution is approximately $\alpha \approx 0.4349$ from using our function defined earlier.

\begin{align*}
  \int_{1}^{\infty}\beta^{-2}d\beta &= \biggl[ -\beta^{-1} \biggr]_{1}^{\infty} = 0 + 1 = 1
\end{align*}

Hence, our prior is a probability density function.

## Solution to 1.2

The mode for an Inverse-Gamma distribution with parameters $\alpha$ and $\beta$ is defined as $\frac{\beta}{\alpha+1}$. So for $\sigma^{2}$ the prior mode will be $\frac{1}{11}$.

And for the prior mode of $\beta$, we do not have a prior mode, since the function is undefined at $\beta = 0$. 


## Solution to 1.3

```{{stan}}
data {
  int<lower=0> N; // num obs
  vector[N] x;    // obs explanatory
  vector[N] y;    // obs response
}

parameters {
  real beta;           // gradient
  real<lower=0> sigma; // error std.
}

model {
  // Priors
  target += -2 * log(beta); // prior for beta
  target += inv_gamma_lpdf(sigma | 0.1, 0.1); // prior on sigma^2
  
  // Likelihood
  target += normal_lpdf(y | beta * x, sigma);
}
```

## Solution to 1.4

```{r, message=FALSE}
library(rstan)

our_data = list(
  N = 8,
  x = c(2.13, 4.32, 3.60, 0.19, 5.62, 2.86, 4.50, 1.95),
  y = c(4.02, 8.73, 7.33, 0.51, 12.09, 5.99, 8.91, 4.02)
)

setwd('C:/Users/guyro/Desktop/DU/Maths/Y3/bayesian-modelling/assignments/assignment7')

model = stan_model('model1.stan')
fit = sampling(model, our_data, chains=4, iter=10000, warmup=500, thin=1)

print(fit, pars=c("beta", "sigma"))

# Extract the posterior samples for beta and sigma
beta_samples = extract(fit)$beta
sigma_samples = extract(fit)$sigma
sigma2_samples = sigma_samples^2

hist(beta_samples)
hist(sigma2_samples)
```


## Solution to 2.1

```{{stan}}
data {
  int<lower=0> N;
  int<lower=0> x[N];
}

parameters {
  real<lower=0> lambda;
}

model {
  // Prior
  lambda ~ frechet(2, 4);
  
  // Likelihood (Comment out to sample from preposterior)
  //for (i in 1:N) {
  //  x[i] ~ poisson(lambda[i]);
  //}
}
```

## Solution to 2.2

```{r, message=FALSE}
library(rstan)
N = 3 # no. data points
x = c(1, 3, 2) # some random count data
our_data = list(N = N, x = x)

model2 = stan_model('model2.stan')
fit = sampling(model2, our_data, iter=10000)
samples = extract(fit, pars=c('x_prepost[1]', 'x_prepost[2]', 'x_prepost[3]'))
plot(density(samples$`x_prepost[1]`))
```

## Solution to 2.3

One solution would be to add some constraints to the model. From the lectures in Chapter 10, we could truncate our model, in order to force the generated samples to be greater than 4. Essentially adding a lower bound. However, this would mean that no samples would be less than 4, which isn't exactly what we want.

Alternatively, we could modify our prior distribution for $lambda$ to "shift" the density further from 4. For example, we use the location parameter, $m$ of the Frechet distribution. Though I am not entirely sure how to implement this in Stan, since the distribution only takes in the scale and shape parameters, not the location parameter, $m$. 
