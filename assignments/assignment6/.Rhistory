zbatch <- matrix(data[J,],1)
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * numDeriv::grad(ext_func, w)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
online_sgd(w, z_obs, 1000, n_obs)
batch_sgd = function(w, data, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = TRUE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * numDeriv::grad(ext_func, w)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
batch_sgd(w, z_obs, 3000, 16, n_obs)
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = FALSE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w + lrate * ((n_obs/batch_size) * numDeriv::grad(f, w) +
log_prior_pdf(w=w, mu=rep(0, 2), Sig2=100*diag(2))) +
sqrt(lrate)*sqrt(tau)*rnorm(2)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
wvals = batch_sgd_prior(w, z_obs, 1.0, 3000, 1000, n_obs)
plot(wvals[, 1])
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
lrate = learning_rate(t, 0.3*num_iters, 0.6*num_iters, 1e-3, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = FALSE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w + lrate * ((n_obs/batch_size) * numDeriv::grad(f, w) +
log_prior_pdf(w=w, mu=rep(0, 2), Sig2=100*diag(2))) +
sqrt(lrate)*sqrt(tau)*rnorm(2)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
w_seed = c(-1, 0)
w_chain = batch_sgd_prior(w_seed, z_obs, 1.0, 1000, 1000, n_obs)
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
1e-3
10^(-1)
10^(-2)
1e-2
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
lrate = learning_rate(t, 0.3*num_iters, 0.6*num_iters, 1e-2, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = FALSE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w + lrate * ((n_obs/batch_size) * numDeriv::grad(f, w) +
log_prior_pdf(w=w, mu=rep(0, 2), Sig2=100*diag(2))) +
sqrt(lrate)*sqrt(tau)*rnorm(2)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
w_seed = c(-1, 0)
w_chain = batch_sgd_prior(w_seed, z_obs, 1.0, 1000, 1000, n_obs)
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
w_seed = c(-1, 0)
w_chain = batch_sgd_prior(w_seed, z_obs, 1.0, 1000, 1000, n_obs)
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs, clipping_threshold) {
w_chain = c(w)
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
lrate = learning_rate(t, 0.3*num_iters, 0.6*num_iters, 1e-2, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = FALSE)
# 2. Calculate the likelihood (over all samples)
grad_est_lik = rep(0.0, times=length(w))
for (j in J) {
f = function(w, z=z_obs[j,]) {
return(log_sampling_pdf(z, w))
}
grad_est_lik = grad_est_lik + numDeriv::grad(f, w)
}
grad_est_lik = (n_obs / batch_size) * grad_est_lik
# 3. Apply clipping (in case gradients explode!)
norm_grad_est_lik = sqrt(sum(grad_est_lik^2))
grad_est_lik = grad_est_lik * min(1.0, clipping_threshold/norm_grad_est_lik)
w = w + lrate*grad_est_lik
# 4. Add the prior pdf
f = function(w, z=zbatch) {
d = length(w)
return(log_prior_pdf(w=w, mu=rep(0, 2), Sig2=100*diag(d)))
}
w = w + lrate*numDeriv::grad(f, w)
# 5. Add the noise
w = w + sqrt(lrate)*sqrt(tau)*rnorm(length(w))
# 6. Store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
w_seed = c(-1, 0)
w_chain = batch_sgd_prior(w_seed, z_obs, 1.0, 1000, 1000, n_obs)
w_chain = batch_sgd_prior(w_seed, z_obs, 1.0, 1000, 1000, n_obs, 10)
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
plot(w_chain[,2], type='l') +
abline(h=w_true[2], col='red')
set.seed(2023)
rnorm(2)
rnorm(2, 0, 1)
rnorm(2, 0, 1)
rnorm(2, 0, 1)
rnorm(2, 0, 1)
set.seed(2023)
rnorm(2, 0, 1)
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs, clipping_threshold) {
w_chain = c(w)
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
lrate = learning_rate(t, 0.3*num_iters, 0.6*num_iters, 1e-2, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = FALSE)
# 2. Calculate the likelihood (over all samples)
grad_est_lik = rep(0.0, times=length(w))
for (j in J) {
f = function(w, z=z_obs[j,]) {
return(log_sampling_pdf(z, w))
}
grad_est_lik = grad_est_lik + numDeriv::grad(f, w)
}
grad_est_lik = (n_obs / batch_size) * grad_est_lik
# 3. Apply clipping (in case gradients explode!)
norm_grad_est_lik = sqrt(sum(grad_est_lik^2))
grad_est_lik = grad_est_lik * min(1.0, clipping_threshold/norm_grad_est_lik)
w = w + lrate*grad_est_lik
# 4. Add the prior pdf
f = function(w, z=zbatch) {
d = length(w)
return(log_prior_pdf(w=w, mu=rep(0, d), Sig2=100*diag(d)))
}
w = w + lrate*numDeriv::grad(f, w)
# 5. Add the noise
w = w + sqrt(lrate)*sqrt(tau)*rnorm(length(w))
# 6. Store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
set.seed(2023)
w_seed = c(-1, 0)
w_chain = batch_sgd_prior(w_seed, z_obs, 1.0, 1000, 1000, n_obs, 10)
plot(w_chain[,1], type='l') +
abline(h=w_true[1], col='red')
plot(w_chain[,2], type='l') +
abline(h=w_true[2], col='red')
library(nnet)
library("faraway")
install.packages("faraway")
library("faraway")
data(ozone)
?ozone
force(ozone)
View(ozone)
nn.out.1 = nnet(O3 ~ temp + ibh + ibt, ozone, size=2, linout=T)
?nnet
# linout True sets linear output units (which we want!)
# size 2 (2 layers)
# and then we define the formula as a linear combination (since)
nn.out.1 = nnet(O3 ~ temp + ibh + ibt, ozone, size=2, linout=T)
compound_dist = function(N, alpha) {
return(beta(alpha+1, N+1)/(beta(alpha, 1)))
}
compound_dist(10, 15)
compound_dist(10, 20)
compound_dist(10, 50)
compound_dist(10, 500)
compound_dist(10, 11)
compound_dist(2, 11)
compound_dist(2, 5)
compound_dist(2, 3)
compound_dist(2, 2)
compound_dist(2, 1)
compound_dist(2, 0)
compound_dist(2, 1)
compound_dist(2, 4)
compound_dist(2, 6)
compound_dist(2, 8)
compound_dist(2, 100)
compound_dist(2, 10000)
compound_dist(2, 1)
# manual test
(factorial(N + alpha) / (factorial(N)*factorial(alpha))) * (N+1)/(alpha+N+1)
N = 2
alpha = 1
compound_dist(N, alpha)
# manual test
(factorial(N + alpha) / (factorial(N)*factorial(alpha))) * (N+1)/(alpha+N+1)
compound_dist = function(N, alpha) {
pmf_N = choose(N + alpha, N) * beta(alpha+1, N+1)/beta(alpha, 1)
return(pmf_N)
}
N = 2
alpha = 1
compound_dist(N, alpha)
# manual test
(factorial(N + alpha) / (factorial(N)*factorial(alpha))) * (N+1)/(alpha+N+1)
alpha = 3
compound_dist(N, alpha)
# manual test
(factorial(N + alpha) / (factorial(N)*factorial(alpha))) * (N+1)/(alpha+N+1)
beta(2, 3)
beta(3, 2)
beta(100, 2)
beta(2, 100)
(gamma(2)*gamma(100))/gamma(102)
compound_dist = function(N, alpha) {
pmf_N = beta(alpha+1, N+1)/beta(alpha, 1) * factorial(N+alpha)
return(pmf_N)
}
N = 2
alpha = 3
compound_dist(N, alpha)
compound_dist = function(N, alpha) {
pmf_N = (beta(N+1, alpha+1)/beta(alpha, 1)) * factorial(N+alpha)
return(pmf_N)
}
N = 2
alpha = 3
compound_dist(N, alpha)
# manual test
(factorial(N + alpha) / (factorial(N)*factorial(alpha))) * (N+1)/(alpha+N+1)
compound_dist = function(N, alpha) {
pmf_N = beta(alpha+1, N+1)/(beta(alpha, 1) * factorial(N+alpha))
return(pmf_N)
}
N = 2
alpha = 3
compound_dist(N, alpha)
N = 2
alpha = 10
compound_dist(N, alpha)
alpha = 1
compound_dist(N, alpha)
beta(3,2)
compound_dist = function(N, alpha) {
pmf_N = pbeta(alpha+1, N+1)/pbeta(alpha, 1)
return(pmf_N)
}
N = 2
alpha = 1
compound_dist(N, alpha)
pmf_N = dbeta(alpha+1, N+1)/pbeta(alpha, 1)
pmf_N = beta(alpha+1, N+1)/pbeta(alpha, 1)
pmf_N = dbeta(alpha+1, N+1)/dbeta(alpha, 1)
compound_dist = function(N, alpha) {
numerator = beta(N+1, alpha+1)
denominator = beta(alpha, 1) * factorial(N + alpha)
return(numerator/denominator)
}
N = 2
alpha = 1
compound_dist(N, alpha)
compound_dist = function(N, alpha) {
numerator = beta(N+1, alpha+1)
denominator = beta(alpha, 1) * factorial(N + alpha)
return(numerator/denominator)
}
compound_dist(2, 1)
compound_dist = function(N, alpha) {
numerator = beta(N+1, alpha+1)
denominator = beta(alpha, 1) * factorial(N + alpha)
return(numerator/denominator)
}
compound_dist(2, 1)
View(compound_dist)
compound_dist <- function(N, alpha) {
numerator <- beta(N+1, alpha+1)
denominator <- beta(alpha, 1) * factorial(N + alpha)
return(numerator/denominator)
}
compound_dist(2, 1)
compound_dist <- function(N, alpha) {
numerator <- stats::beta(N+1, alpha+1)
denominator <- stats::beta(alpha, 1) * factorial(N + alpha)
return(numerator/denominator)
}
compound_dist(2, 1)
compound_dist(1, 1)
compound_dist = function(N, alpha) {
return(beta(alpha+1, N+1) / beta(alpha, 1))
}
compound_dist(2, 1)
compound_dist(1, 1)
compound_dist(0, 1)
compound_dist(2, 2)
compound_dist(1, 2)
compound_dist(0, 2)
compound_dist(2, 2)
compound_dist(1, 2)
compound_dist(2, 1)
compound_dist(1, 1)
compound_dist(2, 2)
compound_dist(1, 2)
compound_dist(1, 3)
compound_dist(2, 3)
compound_dist(2, 4)
compound_dist(2, 4
)
compound_dist(2, 4)
compound_dist(1, 4)
compound_dist(2, 4)
compound_dist(1, 4)
compund_dist(0, 4)
compound_dist(0, 4)
compound_dist(2, 4)
compound_dist(1, 4)
compound_dist(0, 4)
compound_dist(0, 1)
compound_dist(0, 2)
compound_dist(0, 5)
compound_dist(0, 9)
compound_dist(0, 1)
compound_dist(2, 1)
compound_dist(1, 1)
compound_dist(0, 1)
compound_dist(2, 0)
compound_dist(1, 0)
compound_dist(0, 0)
compound_dist(0, 1)
pi_N(2, 1)
pi_N <- function(N, alpha) {
return(factorial(N-1) / ((alpha-1) * factorial(N+alpha-1)))
}
pi_N(2, 1)
pi_N(2, 1)
pi_N(4, 4)
pi_N(2, 0)
pi_N(2, 5)
pi_N(2, 10)
pi_N(5, 5)
# Define function to find difference between Pr(N<=2) and 0.5
diff_fun <- function(alpha) {
p_N2 <- pi_N(0, alpha) + pi_N(1, alpha) + pi_N(2, alpha)
return(p_N2 - 0.5)
}
# Find value of alpha that makes Pr(N<=2) approximately 0.5
root <- uniroot(diff_fun, interval=c(0, 10))
alpha <- root$root
a = compound_dist(2, 1)
b = compound_dist(1, 1)
c = compound_dist(0, 1)
a+b+c
a = compound_dist(2, 2)
b = compound_dist(1, 2)
c = compound_dist(0, 2)
a+b+c
a = compound_dist(2, 0.5)
b = compound_dist(1, 0.5)
c = compound_dist(0, 0.5)
a+b+c
a = compound_dist(2, 0.4)
b = compound_dist(1, 0.4)
c = compound_dist(0, 0.4)
a+b+c
a = compound_dist(2, 0.45)
b = compound_dist(1, 0.45)
c = compound_dist(0, 0.45)
a+b+c
a = compound_dist(2, 0.44)
b = compound_dist(1, 0.44)
c = compound_dist(0, 0.44)
a+b+c
a = compound_dist(2, 0.435)
b = compound_dist(1, 0.435)
c = compound_dist(0, 0.435)
a+b+c
gamma(1, 1)
pgamma(1, 1)
pgamma(1, 1)
pgamma(1, 1)
pgamma(1, 1)
# PART 2
setwd('C:/Users/guyro/OneDrive/Y3/bayesian-modelling/assignments/assignment6')
# PART 2
setwd('C:/Users/guyro/OneDrive/Y3/bayesian-modelling/assignments/assignment6/')
getwd()
# PART 2
setwd('C:/Users/guyro/OneDrive/Y3/bayesian-modelling')
# PART 2
setwd('C:/Users/guyro/OneDrive/Y3/bayesian-modelling/')
setwd()
setwd('C:/Users/guyro/OneDrive/Y3/bayesian-modelling')
# PART 2
setwd('C:\Users\guyro\Desktop\DU\Maths\Y3\bayesian-modelling\assignments\assignment6')
# PART 2
setwd('C:/Users/guyro/Desktop/DU/Maths/Y3/bayesian-modelling/assignments/assignment6')
model = stan_model('model.stan')
# PART 2
library(rstan)
setwd('C:/Users/guyro/Desktop/DU/Maths/Y3/bayesian-modelling/assignments/assignment6')
model = stan_model('model.stan')
rstan:::rstudio_stanc("model.stan")
# PART 2
library(rstan)
setwd('C:/Users/guyro/Desktop/DU/Maths/Y3/bayesian-modelling/assignments/assignment6')
model = stan_model('model.stan')
X = c(3.12, 0.75, 5.46, 0.80, 3.42,
3.11, 1.99, 2.86, 4.33, 2.98)
data = list(n = 10,
X=X)
test_sample = sampling(model, data)
summary(test_sample)
hist(test_sample)
View(test_sample)
names(test_sample)
hist(test_sample$mu)
hist(test_sample$mu)
test_sample$mu
test_sample[1]
test_sample
sample = extract(test_sample)
sample
hist(sample)
sample$mu
hist(sample$mu)
# Sample from posterior
posterior <- sampling(model, data = list(X = X, n = n), iter = 10000, chains = 4)
n = length(X)
# Sample from posterior
posterior <- sampling(model, data = list(X = X, n = n), iter = 10000, chains = 4)
# Extract posterior samples
mu_samples <- extract(posterior)$mu
# Summary statistics
summary(mu_samples)
# Histogram
hist(mu_samples, breaks = 30)
