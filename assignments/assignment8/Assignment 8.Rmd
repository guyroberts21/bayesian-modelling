---
title: "Solutions for assignment 8"
author: "Guy Roberts"
date: "2023-03-23"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Solution to 1.1

```{{stan}}
data {
  int<lower=0> N;
  int<lower=0> h[N];
}

parameters {
  real<lower=0> lambda;
}

model {
  // Prior
  lambda ~ gamma(2, 2);
  
  // Likelihood
  for (i in 1:N) {
    h[i] ~ poisson(lambda);
  }
}

generated quantities {
  vector[N] log_lik;
  for (i in 1:N) 
    log_lik[i] = poisson_lpmf(h[i] | lambda); 
}
```

```{{stan}}
data {
  int<lower=0> N;
  int<lower=0> h[N];
}

parameters {
  real<lower=0> lambda;
}

model {
  // Prior
  lambda ~ gamma(2, 2);
  
  // Likelihood
  for (i in 1:N) {
    h[i] ~ binomial(100, lambda/100);
  }
}

generated quantities {
  vector[N] log_lik;
  for (i in 1:N) 
    log_lik[i] = binomial_lpmf(h[i] | 100, lambda/100); 
}
```

```{{stan}}
data {
  int<lower=0> N;
  vector[N] h;
}

parameters {
  real<lower=0> lambda;
}

model {
  // Prior
  lambda ~ gamma(2, 2);
  
  // Likelihood
  for (i in 1:N) {
    h[i] ~ normal(lambda, (lambda*(100-lambda))/100);
  }
}

generated quantities {
  vector[N] log_lik;
  for (i in 1:N) 
    log_lik[i] = normal_lpdf(h[i] | lambda, (lambda*(100-lambda))/100); 
}
```

## Solution to 1.2

```{r, message=FALSE, cache=TRUE}
library(rstan)
library(ggplot2)
our_data = list(N = 20,
                h = c(1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0))
setwd(
  'C:/Users/guyro/OneDrive/Y3/bayesian-modelling/assignments/assignment8'
)

# Load the Stan models into R
model1 = stan_model('model1.stan')
model2 = stan_model('model2.stan')
model3 = stan_model('model3.stan')

# Sample from the posterior (fit the models)
fit1 = sampling(model1, our_data, iter = 10000)
fit2 = sampling(model2, our_data, iter = 10000)
fit3 = sampling(model3, our_data, iter = 10000)


# Extract the lambdas
lambda1 = extract(fit1)$lambda
lambda2 = extract(fit2)$lambda
lambda3 = extract(fit3)$lambda

# Plot the posterior samples
dat = data.frame(var = factor(rep(c(
  'lambda1', 'lambda2', 'lambda3'
), each=20000)),
values = c(lambda1, lambda2, lambda3))

ggplot(dat, aes(x = values, color=var)) + geom_density(size=1.15)
```

## Solution to 1.3

```{r, message=FALSE, cache=TRUE}
library(loo)
# Calculate pseudo-BMA weights
loo1 = loo(fit1)
loo2 = loo(fit2)
loo3 = loo(fit3)

# Print some of the weights
loo2
loo3

loo_model_weights(list(loo1, loo2, loo3),
                  method = "pseudobma",
                  BB = FALSE)
```

- This [link](https://discourse.mc-stan.org/t/a-quick-note-what-i-infer-from-p-loo-and-pareto-k-values/3446) provides some insight on the output from the LOO fits.

NOTE: We get a warning about some Pareto k diagnostic values being slightly high - One value is in the range $(0.5, 0.7]$ (ok) : which may indicate that the model is badly mis-specified (see link).

Considering the sample densities of the posterior (shown above), these results do match our expectations. The graph shows that models 1 and 2 are very similar, with mode ~ 0.4, whereas the mode for model 3 ~ 0.5 and this model has a lower spread than the others, and hence a higher peak. The results also suggest that model 1 is marginally more suitable (given the data) than model 2 - however, the dataset used is small, so perhaps more in-depth analysis is required in order to make an appropriate decision. 

## Solution to 2.1

Here are the models:

\begin{align*}
    M_1: \qquad &Y|\alpha \sim \operatorname{Exp}(\alpha) \\
    M_2: \qquad &Y|\nu \sim \chi^2(\nu) \\
    M_3: \qquad &\sqrt{Y}|\beta \sim \operatorname{Maxwell-Boltzmann}(\beta)
\end{align*}

Now if we consider the PDFs of each distribution:

\begin{align}
    \pi_{M_1}(y) &\propto \exp(-y) \\
    \pi_{M_2}(y) &\propto y^{\frac{\nu}{2} - 1}\exp(-\frac{y}{2}) \\
    \pi_{M_3}(y) &\propto y\exp(-y)
\end{align}

Then we can see that the similarity between these distributions is that they are can all be formed from Gamma distributions: Gamma(1, 1), Gamma($\frac{\nu}{2}$, $\frac{1}{2}$) and Gamma(2, 1) respectively.

## Solution to 2.2

As mentioned, the priors could be Gamma(1, 1), Gamma($\frac{\nu}{2}$, $\frac{1}{2}$) and Gamma(2, 1).

```{{stan}}
data {
  int<lower=0> N; // num obs
}

parameters {
  real<lower=0> alpha;          // rate param for exp distribution
  real<lower=0> nu;             // df param for chisq distribution
  real<lower=0> beta;           // scale parameter for MB distribution
  real<lower=0,upper=1> p1;     // exp distribution (mixing prop.)
  real<lower=0,upper=1-p1> p2;  // chisq distribution (mixing prop.)
}

model {
  // Priors
  alpha ~ gamma(1, 1);
  nu ~ gamma(0.5*nu, 0.5);
  beta ~ gamma(2, 1);
}

generated quantities {
  real<lower=0> y_sim[N];
  
  for (n in 1:N) {
    if (uniform_rng(0, 1) < p1) {
      // exponential
      y_sim[n] = exponential_rng(alpha);
    } else if (uniform_rng(0, 1) < p1 + p2) {
      // chi-squared
      y_sim[n] = chi_square_rng(nu);
    } else {
      // Maxwell-Boltzmann
      real z = normal_rng(0, 1);
      real y = sqrt(2 / (pi() * beta)) * exp(-z^2 / 2);
      y_sim[n] = y^2; // squared since we are given sqrt(Y) ~ MB
    }
  }
}
```

```{r, message=FALSE, cache=TRUE}
model4 = stan_model('model4.stan')

num_samples = 1000
#samples = sampling(model4, data=list(N=num_samples), iter=10000)

#y_sample = extract(samples, par='y_sim')$y_sim
#summary(y_sample)
```

This code is extremely slow and seems to give the wrong output - I haven't had enough time to fully understand what is happening hence the incompleteness. So it is commented out.

We can additionally modify the priors so that they accommodate this new information. For example, for the exponential distribution in $M_1$ we can use a Gamma(2, 1) which will give the prior belief that Y is on average 2.

