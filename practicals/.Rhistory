p <- exp(p) / (1+exp(p))
z[,3] <- rbinom(n, size = 1, prob = p)
return(z)
}
n_obs <- 10^(6)
w_true <- c(0,1)
set.seed(2023)
z_obs <- data_generating_model(n = n_obs, w = w_true)
set.seed(0)
w_true <- as.numeric(glm(z_obs[,3]~ 1 + z_obs[,2],family = "binomial" )$coefficients)
log_sampling_pdf <- function(z, w) {
d <- length(w)
x <- z[1:d]
y <- z[d+1]
log_pdf <- y * log(prediction_rule(x,w)) +(1-y) * log( 1.0-prediction_rule(x,w) )
#log_pdf <- dbinom(y, size = 1, prob = prediction_rule(x,w), log = TRUE)
return( log_pdf )
}
w = c(-0.1, 1.5)
numDeriv::grad(log_sampling_pdf(z_obs, w))
prediction_rule <- function(x,w) {
h <- w[1]*x[1]+w[2]*x[2]
h <- exp(h) / (1.0 + exp(h) )
return (h)
}
w = c(-0.1, 1.5)
numDeriv::grad(log_sampling_pdf(z_obs, w))
numDeriv::grad(ext_func(ext_func, w))
ext_func = function(z, w) {
log_sampling_pdf(z, w)
}
w = c(-0.1, 1.5)
numDeriv::grad(ext_func(ext_func, w))
numDeriv::grad(ext_func, w)
w = c(-0.1, 1.5)
ext_func = function(z=z_obs, w=w) {
log_sampling_pdf(z, w)
}
numDeriv::grad(ext_func, w)
ext_func = function(z=z_obs[1, ], w=w) {
log_sampling_pdf(z, w)
}
numDeriv::grad(ext_func, w)
ext_func = function(z=z_obs[1, ], w) {
log_sampling_pdf(z, w)
}
numDeriv::grad(ext_func, w)
View(z_obs)
w = c(-0.1, 1.5)
ext_func = function(z=z_obs, w) {
return(log_sampling_pdf(z, w))
}
numDeriv::grad(ext_func, w)
w = c(-0.1, 1.5)
ext_func = function(w, z=z_obs[1, ]) {
return(log_sampling_pdf(z, w))
}
numDeriv::grad(ext_func, w)
grad_log_sampling_pdf <- function(w,z) {
d <- length(w)
x = z[1:d]
y = z[d+1]
h <- prediction_rule(x,w)
grd <- -(h-y)*x
return (grd)
}
gr <- grad_log_sampling_pdf(w,z = z_obs[1,])
gr
sgd_online = function(w, data, num_iters, lrate, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = TRUE)
zbatch <- matrix(data[J,],1,2)
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * numDeriv::grad(ext_func, w)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
sgd_online(w, z_obs, 1000, 0.1, n_obs)
sgd_online = function(w, data, num_iters, lrate, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = 1, replace = TRUE)
zbatch <- matrix(data[J,],1,2)
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * numDeriv::grad(ext_func, w)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
sgd_online(w, z_obs, 1000, 0.1, n_obs)
sgd_online = function(w, data, num_iters, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = 1, replace = TRUE)
zbatch <- matrix(data[J,],1,2)
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * numDeriv::grad(ext_func, w)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
sgd_online(w, z_obs, 1000, n_obs)
learning_rate <- function(t, T_0 = 100, T_1 = 500, C_0 = 0.0001, s_0 = 0.5 ) {
if ( t <= T_0 ) {
eta <- C_0
} else if ( (T_0+1 <= t) && (t <= T_1 ) ) {
eta <- C_0 / ( (t-T_0) ^ s_0 )
} else {
eta <- C_0 / ( (T_1-T_0) ^ s_0 )
}
return(eta)
}
sgd_online = function(w, data, num_iters, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = 1, replace = TRUE)
zbatch <- matrix(data[J,],1,2)
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * numDeriv::grad(ext_func, w)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
sgd_online(w, z_obs, 1000, n_obs)
warnings()
print(data[5, ])
print(z_obs[5, ])
print(z_obs[5, ], 1)
print(z_obs[5, ], 1, 2)
zbatch <- matrix(data[J,],1)
sgd_online = function(w, data, num_iters, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = 1, replace = TRUE)
zbatch <- matrix(data[J,],1)
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * numDeriv::grad(ext_func, w)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
sgd_online(w, z_obs, 1000, n_obs)
matrix(z_obs[2, ], 1)
matrix(z_obs[2, ])
matrix(z_obs[2, ], 1)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac2_term2.R", echo=TRUE)
batch_sgd(w, data, num_iters, batch_size, n_obs) {
batch_sgd = function(w, data, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = TRUE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * numDeriv::grad(ext_func, w)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
batch_sgd(w, z_obs, 3000, 16, n_obs)
log_prior_pdf <- function(w, mu, Sig2 ) {
log_pdf <- dmvnorm(w, mean = mu, sigma = Sig2, log = TRUE, checkSymmetry = TRUE)
return( log_pdf )
}
View(z_obs)
100*diag(2)
sqrt(2)
rnorm(1)
rnorm(1)
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = TRUE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * ((n_obs/batch_size) * numDeriv::grad(ext_func, w) +
log_prior_pdf(w=w, mu=0, Sig2=100*diag(2))) +
sqrt(lrate*tau*rnorm(1))
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
batch_sgd_prior(w, z_obs, 1.0, 500, 10, n_obs)
dmvnorm(c(-1, -2), rep(0, 2), 100*diag(2))
rep(0, 2)
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = TRUE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * ((n_obs/batch_size) * numDeriv::grad(ext_func, w) +
log_prior_pdf(w=w, mu=rep(0, 2), Sig2=100*diag(2))) +
sqrt(lrate*tau*rnorm(1))
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
batch_sgd_prior(w, z_obs, 1.0, 500, 10, n_obs)
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = TRUE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * ((n_obs/batch_size) * numDeriv::grad(ext_func, w) +
log_prior_pdf(w=w, mu=rep(0, 2), Sig2=100*diag(2))) +
sqrt(lrate)*sqrt(tau*rnorm(1))
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
batch_sgd_prior(w, z_obs, 1.0, 500, 10, n_obs)
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = TRUE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * ((n_obs/batch_size) * numDeriv::grad(ext_func, w) +
log_prior_pdf(w=w, mu=rep(0, 2), Sig2=100*diag(2))) +
sqrt(lrate)*sqrt(tau)*rnorm(1)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
batch_sgd_prior(w, z_obs, 1.0, 500, 10, n_obs)
Tmax <- 3*10^(3)
#
w_seed <- c(-1,0)
#
eta <- 10^(-6)
eta_C <- eta
eta_s <- 0.51
eta_T0 <- 0.3*Tmax
eta_T1 <- 0.6*Tmax
#
batch_size <- 1000
#
tau <- 1.0
#
# Set the seed
w <- w_seed
w_chain <- c(w)
# iterate
t <- 1
Qterm <- 0
#
# iterate
#
while ( (Qterm != 1) ) {
# counter
t <- t+1
# learning rate
eta <- learning_rate(t, eta_T0, eta_T1, eta_C, eta_s)
# sub-sample
J <- sample.int( n = n_obs, size = batch_size, replace = FALSE)
# update
## likelihood
grad_est_lik <- rep( 0.0, times=length(w) )
for (j in J) {
aux_fun <- function(w, z=z_obs[j,]){
gr <- log_sampling_pdf(z, w)
return(gr)
}
grad_est_lik <- grad_est_lik + numDeriv::grad(aux_fun, w)
}
grad_est_lik <- ( n_obs / batch_size) * grad_est_lik
w <- w +eta*grad_est_lik ;
## prior
aux_fun <- function(w){
d <- length(w)
gr <- log_prior_pdf(w, rep(0,d), 100*diag(d))
return(gr)
}
w <- w +eta*numDeriv::grad(aux_fun, w) ;
## noise
w <- w +sqrt(eta)*sqrt(tau)*rnorm(n = length(w), mean = 0, sd = 1)
# termination criterion
if  ( t >= Tmax ) {
Qterm <- 1
}
# record the produced chain
w_chain <- rbind(w_chain,w)
}
Tmax <- 3*10^(3)
print(Tmax)
#
w_seed <- c(-1,0)
#
eta <- 10^(-6)
eta_C <- eta
eta_s <- 0.51
eta_T0 <- 0.3*Tmax
eta_T1 <- 0.6*Tmax
#
batch_size <- 1000
#
tau <- 1.0
#
# Set the seed
w <- w_seed
batch_sgd_prior(w, z_obs, 1.0, 3000, 1000, n_obs)
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = TRUE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * ((n_obs/batch_size) * numDeriv::grad(f, w) +
log_prior_pdf(w=w, mu=rep(0, 2), Sig2=100*diag(2))) +
sqrt(lrate)*sqrt(tau)*rnorm(2)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
batch_sgd_prior(w, z_obs, 1.0, 3000, 1000, n_obs)
view(w_chain)
View(w_chain)
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = FALSE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w - lrate * ((n_obs/batch_size) * numDeriv::grad(f, w) +
log_prior_pdf(w=w, mu=rep(0, 2), Sig2=100*diag(2))) +
sqrt(lrate)*sqrt(tau)*rnorm(2)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
batch_sgd_prior(w, z_obs, 1.0, 3000, 1000, n_obs)
wvals = batch_sgd_prior(w, z_obs, 1.0, 3000, 1000, n_obs)
plot(wvals[, 1])
## Step 3 - BATCH SGD with PRIOR
batch_sgd_prior = function(w, data, tau, num_iters, batch_size, n_obs) {
w_chain = c()
for (t in 1:num_iters) {
# Get the current learning rate: note; this function gradually returns
# a learning rate which gradually decreases
T_half = floor(num_iters/2)
lrate = learning_rate(t, T_half, num_iters, 1e-6, 0.51)
# 1. get random sample of size {batch_size} from the data
J <- sample.int(n = n_obs, size = batch_size, replace = FALSE)
zbatch = 0
if (batch_size == 1) {
zbatch <- matrix(data[J,],1)
}
else {
zbatch <- data[J,]
}
f = function(w, z=zbatch) {
return(log_sampling_pdf(z, w))
}
# 2. update parameters - again, we use the higher order function for our gradient calculation
w = w + lrate * ((n_obs/batch_size) * numDeriv::grad(f, w) +
log_prior_pdf(w=w, mu=rep(0, 2), Sig2=100*diag(2))) +
sqrt(lrate)*sqrt(tau)*rnorm(2)
# store result in chain
w_chain = rbind(w_chain, w)
}
return(w_chain)
}
wvals = batch_sgd_prior(w, z_obs, 1.0, 3000, 1000, n_obs)
plot(wvals[, 1])
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
install.packages("rmarkdown")
install.packages("tinytex")
tinytex::install_tinytex()
setwd('C:\Users\guyro\OneDrive\Y3\bayesian-modelling\practicals')
source("~/.active-rstudio-document", echo=TRUE)
our_model <- stan_model('conjugate_1.stan')
rstan:::rstudio_stanc("conjugate_1.stan")
our_model <- stan_model('conjugate_1.stan')
example(stan_model, package = "rstan", run.dontrun = TRUE)
