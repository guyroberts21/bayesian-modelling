?extract
knitr::opts_chunk$set(echo = TRUE)
hist(samples)
samples <- extract(fit, pars = c('lambda[1]','lambda[2]'))
print(samples)
hist(samples)
nn.out.1
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac3_term_2.R", echo=TRUE)
nn.out.1
names(nn.out.1)
sum((nn.out.1$fitted.values-ozone$O3)^2)
# RSS
sum((ozone$O3 - mean(ozone$O3))^2)
apply(ozone, 2, sd)
?scale
scaled_data = scale(ozone, 0, 1)
scaled_data = scale(ozone, 0, 1)
scaled_data = scale(ozone)
mean(scaled_data)
var(scaled_data)
mean(scaled_data)
mean(scaled_data$O3)
names(scaled_data)
apply(ozone.rescaled, 2, mean)
ozone.rescaled = scale(ozone)
apply(ozone.rescaled, 2, mean)
# Find the best model over n iterations
n = 100
nn.out.1.best = nn.out.1
for (i in 1:n) {
set.seed(i)
nn.out.1.curr = nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T)
err = sum((nn.out.1.curr$fitted.values-ozone$O3)^2)
if (err < nn.out.1.best)
nn.out.1.best = nn.out.1.curr
}
# Find the best model over n iterations
n = 100
nn.out.1.best = init_err
for (i in 1:n) {
set.seed(i)
nn.out.1.curr = nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T)
err = sum((nn.out.1.curr$fitted.values-ozone$O3)^2)
if (err < nn.out.1.best)
nn.out.1.best = nn.out.1.curr
}
init_err = sum((nn.out.1$fitted.values-ozone$O3)^2)
# Find the best model over n iterations
n = 100
nn.out.1.best = init_err
for (i in 1:n) {
set.seed(i)
nn.out.1.curr = nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T)
err = sum((nn.out.1.curr$fitted.values-ozone$O3)^2)
if (err < nn.out.1.best)
nn.out.1.best = nn.out.1.curr
}
nn.out.1.best
nn.out.1$value
# Find the best model over n iterations
n = 100
nn.out.1.best = nn.out.1
for (i in 1:n) {
set.seed(i)
nn.out.1.curr = nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T)
err = nn.out.1.curr$value
if (err < nn.out.1.best$value)
nn.out.1.best = nn.out.1.curr
}
nn.out.1.best$value
# Find the best model over n iterations
n = 100
nn.out.1.best = nn.out.1
for (i in 1:n) {
set.seed(i)
nn.out.1.curr = nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T)
err = nn.out.1.curr$value
if (err < nn.out.1.best$value)
nn.out.1.best = nn.out.1.curr
}
nn.out.1.best$value
nn.out.1$value
summary(nn.out.1.best)
# Create the input object x that will be used in the function predict()
xx <- expand.grid(temp=seq(-3,3,0.1),ibh=0,ibt=0)
# Make the predictions
pred.1.best <- predict(nn.out.1.best,new=xx)
# Your trained model is scaled, you need to bring its unites (input/output) back to the natural scale
# Here is how you get the mean and variances of the original data set
ozmeans <- attributes(ozone.rescaled)$"scaled:center"
ozscales <- attributes(ozone.rescaled)$"scaled:scale"
# Apply the re-scaling back to the original data for the inputs
xx.rescaled <- xx$temp*ozscales['temp']+ozmeans['temp']
# Apply the re-scaling back to the original data for the inputs
pred.1.best.rescaled <- pred.1.best*ozscales['O3']+ozmeans['O3']
# plot
plot(xx.rescaled,
pred.1.best.rescaled,
cex=2,xlab="Temp",ylab="O3")
# Create the input object x that will be used in the function predict()
xx <- expand.grid(temp=0,ibh=seq(-3, 3, 0.1),ibt=0)
# plot
plot(xx.rescaled,
pred.1.best.rescaled,
cex=2,xlab="Temp",ylab="O3")
# Create the input object x that will be used in the function predict()
xx <- expand.grid(temp=0,ibh=seq(-3, 3, 0.1),ibt=0)
# Make the predictions
pred.1.best <- predict(nn.out.1.best,new=xx)
# Your trained model is scaled, you need to bring its unites (input/output) back to the natural scale
# Here is how you get the mean and variances of the original data set
ozmeans <- attributes(ozone.rescaled)$"scaled:center"
ozscales <- attributes(ozone.rescaled)$"scaled:scale"
# Apply the re-scaling back to the original data for the inputs
xx.rescaled <- xx$ibh*ozscales['ibh']+ozmeans['ibh']
# Apply the re-scaling back to the original data for the inputs
pred.1.best.rescaled <- pred.1.best*ozscales['O3']+ozmeans['O3']
# plot
plot(xx.rescaled,
pred.1.best.rescaled,
cex=2,xlab="ibh",ylab="O3")
nn.out.1.curr = nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T, decay=0.001)
# Find the best model over n iterations (with lasso shrinkage)
n = 100
nn.out.1.best = nn.out.1
for (i in 1:n) {
set.seed(i)
nn.out.1.curr = nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T, decay=0.001)
err = nn.out.1.curr$value
if (err < nn.out.1.best$value)
nn.out.1.best = nn.out.1.curr
}
nn.out.1.best$value # much better result (compared to original RSS value!)
summary(nn.out.1.best)
# Find the best model over n iterations (with lasso shrinkage)
n = 100
nn.out.1.best = nn.out.1
for (i in 1:n) {
set.seed(i)
nn.out.1.curr = nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T, decay=0.001)
err = nn.out.1.curr$value
if (err < nn.out.1.best$value)
nn.out.1.best = nn.out.1.curr
}
nn.out.1.best$value # much better result (compared to original RSS value!)
summary(nn.out.1.best)
# Find the best model over n iterations (with lasso shrinkage)
n = 100
nn.out.1.best = nn.out.1
for (i in 1:n) {
set.seed(i)
nn.out.1.curr = nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T, decay=0.001)
err = nn.out.1.curr$value
if (err < nn.out.1.best$value)
nn.out.1.best = nn.out.1.curr
}
nn.out.1.best$value # much better result (compared to original RSS value!)
summary(nn.out.1.best)
# Find the best model over n iterations (with lasso shrinkage)
n = 100
nn.out.1.best = nn.out.1
for (i in 1:n) {
set.seed(i)
nn.out.1.curr = nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T, decay=0.001)
err = nn.out.1.curr$value
if (err < nn.out.1.best$value)
nn.out.1.best = nn.out.1.curr
}
nn.out.1.best$value
# Find the best model over n iterations (with lasso shrinkage)
n = 1000
nn.out.1.best = nn.out.1
for (i in 1:n) {
set.seed(i)
nn.out.1.curr = nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T, decay=0.001)
err = nn.out.1.curr$value
if (err < nn.out.1.best$value)
nn.out.1.best = nn.out.1.curr
}
nn.out.1.best$value
# Find the best model over n iterations
n = 100
nn.out.1.best = nn.out.1
for (i in 1:n) {
set.seed(i)
nn.out.1.curr = nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T)
err = nn.out.1.curr$value
if (err < nn.out.1.best$value)
nn.out.1.best = nn.out.1.curr
}
nn.out.1.best$value # much better result (compared to original RSS value!)
# Find the best model over n iterations (with lasso shrinkage)
n = 100
for (i in 1:n) {
set.seed(i)
nn.out.1.curr = nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T, decay=0.001)
err = nn.out.1.curr$value
if (err < nn.out.1.best$value)
nn.out.1.best = nn.out.1.curr
}
nn.out.1.best$value
summary(nn.out.1.best)
# Create the input object x that will be used in the function predict()
xx <- expand.grid(temp=0,ibh=0,ibt=seq(-3,3,0.1))
# Make the predictions
pred.1.best <- predict(nn.out.1.best,new=xx)
# Your trained model is scaled, you need to bring its unites (input/output) back to the natural scale
# Here is how you get the mean and variances of the original data set
ozmeans <- attributes(ozone.rescaled)$"scaled:center"
ozscales <- attributes(ozone.rescaled)$"scaled:scale"
# Apply the re-scaling back to the original data for the inputs
xx.rescaled <- xx$temp*ozscales['ibt']+ozmeans['ibt']
# Apply the re-scaling back to the original data for the inputs
pred.1.best.rescaled <- pred.1.best*ozscales['O3']+ozmeans['O3']
# plot
plot(xx.rescaled,
pred.1.best.rescaled,
cex=2,xlab="IBT",ylab="O3")
# Find the best model over n iterations (with lasso shrinkage)
n <- 100
nn.out.1.decay.best <- nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T)
EF.decay.best <- nn.out.1.best$value
#
for (r in 1:n) {
#
set.seed( r )
#
nn.out.1.new <- nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T, decay=0.001)
EF.new <- nn.out.1.new$value
#
if (EF.new < EF.best) {
#
nn.out.1.decay.best <- nn.out.1.new
#
EF.decay.best <- EF.new
}
}
#
for (r in 1:n) {
#
set.seed( r )
#
nn.out.1.new <- nnet(O3 ~ temp + ibh + ibt, ozone.rescaled, size=2, linout=T, decay=0.001)
EF.new <- nn.out.1.new$value
#
if (EF.new < nn.out.1.best$value) {
nn.out.1.decay.best <- nn.out.1.new
EF.decay.best <- EF.new
}
}
# Create the input object x that will be used in the function predict()
xx <- expand.grid(temp=0,ibh=0,ibt=seq(-3,3,0.1))
# Make the predictions
pred.1.best <- predict(nn.out.1.best,new=xx)
# Your trained model is scaled, you need to bring its unites (input/output) back to the natural scale
# Here is how you get the mean and variances of the original data set
ozmeans <- attributes(ozone.rescaled)$"scaled:center"
ozscales <- attributes(ozone.rescaled)$"scaled:scale"
# Apply the re-scaling back to the original data for the inputs
xx.rescaled <- xx$temp*ozscales['ibt']+ozmeans['ibt']
# Apply the re-scaling back to the original data for the inputs
pred.1.best.rescaled <- pred.1.best*ozscales['O3']+ozmeans['O3']
# plot
plot(xx.rescaled,
pred.1.best.rescaled,
cex=2,xlab="IBT",ylab="O3")
# plot
plot(xx.rescaled,
pred.1.best.rescaled,
cex=2,xlab="IBT",ylab="O3")
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac3_term_2.R", echo=TRUE)
source("C:/Users/guyro/OneDrive/Y3/ml-neural-nets/practicals/prac3_term_2.R", echo=TRUE)
#
# Create the input object x that will be used in the function predict()
#
xx <-expand.grid(temp=0,ibh=0,ibt=seq(-3,3,0.1))
#
# Make the predictions
#
pred.1.best <- predict(nn.out.1.decay.best,new=xx)
#
# Your trained model is scaled, you need to bring its unites (input/output) back to the natural scale
#
# Here is how you get the mean and varances of the original data set
#
ozmeans <- attributes(ozone.rescaled)$"scaled:center"
ozscales <- attributes(ozone.rescaled)$"scaled:scale"
#
# Apply the re-scaling back to the original data for the inputs
#
xx.rescaled <- xx$ibt*ozscales['ibt']+ozmeans['ibt']
#
# Apply the re-scaling back to the original data for the inputs
#
pred.1.best.rescaled <- pred.1.best*ozscales['O3']+ozmeans['O3']
#
# plot
#
plot(xx.rescaled,
pred.1.best.rescaled,
cex=2,xlab="ibt",ylab="O3",
type="l")
pi_function <- function(N, alpha) {
numerator <- beta(N + 1, alpha + 1)
denominator <- beta(alpha, 1) * factorial(N + alpha)
return(numerator / denominator)
}
pi_function(2, 1)
compound_dist <- function(N, alpha) {
numerator <- stats::beta(N+1, alpha+1)
denominator <- stats::beta(alpha, 1) * factorial(N + alpha)
return(numerator/denominator)
}
compound_dist(2, 1)
# List all attached packages
search()
# Detach all attached packages except "base"
# Note: "base" is the core package that is always loaded in R
detach(package: .packages(all.available = TRUE), unload = TRUE, character.only = TRUE)
version
version(R)
R.version
R.Version()
piN <- function(N, alpha) {
num <- factorial(N + alpha - 1) / factorial(N) / factorial(alpha - 1)
den <- beta(alpha, 1)
return(num / den)
}
# Example usage:
piN(2, 1)
# manual test
(factorial(N + alpha) / (factorial(N)*factorial(alpha))) * (N+1)/(alpha+N+1)
piN <- function(N, alpha) {
num <- factorial(N + alpha - 1) / factorial(N) / factorial(alpha - 1)
den <- beta(alpha, 1)
return(num / den)
}
# Example usage:
piN(2, 1)
stats
install.packages(stats)
install.packages("stats")
install.packages("stats")
install.packages("stats")
install.packages("stats")
install.packages("stats")
stats
stats::
stats::beta
stats::beta()
beta(2, 1)
piN(2, 1)
piN <- function(N, alpha) {
return((alpha/(N+alpha)) * (1/(1+alpha))^N)
}
# Example usage:
piN(2, 1)
source("C:/Users/guyro/OneDrive/Y3/bayesian-modelling/assignments/assignment6/question_code.R", echo=TRUE)
beta(3, 2) / beta(1, 1)
compound_dist = function(N, alpha) {
return(beta(alpha+1, N+1) / beta(alpha, 1))
}
compound_dist(2, 1)
library(rstan)
rstan:::rstudio_stanc("C:/Users/guyro/OneDrive/Y3/bayesian-modelling/practicals/prac3_stan.stan")
rstan:::rstudio_stanc("C:/Users/guyro/OneDrive/Y3/bayesian-modelling/practicals/prac3_stan.stan")
rstan:::rstudio_stanc("C:/Users/guyro/OneDrive/Y3/bayesian-modelling/practicals/prac3_stan.stan")
setwd('C:/Users/guyro/OneDrive/Y3/bayesian-modelling/practicals')
model = stan_model('prac3_stan.stan')
our_sample = sampling(model, data = list(n=7, X = c(1.20, 1.21, 3.06, 7.89, 5.67, 6.10, 3.90)))
summary(our_sample)
mu_sample <- extract(our_sample,
pars = 'mu')$`mu`
mu_sample
hist(mu_sample)
summary(our_sample)
rstan:::rstudio_stanc("prac3_stan.stan")
sum(c(1.20, 1.21, 3.06, 7.89, 5.67, 6.10, 3.90))/8
model = stan_model('prac3_stan.stan')
# ==== 2.2 A BIZARRE PRIOR DENSITY ==== #
our_data <- list(n = 20,
p = c(0.25057883,0.16862872,0.11989827,0.28149519,0.20427907,
0.16859187,0.40253736,0.09341611,0.14762340,0.14047014,
0.29998209,0.19349593,0.21179227,0.24900885,0.32570937,
0.12341203,0.20488021,0.33726469,0.08214418,0.41775598))
our_sample = sampling(model, data = list(n=7, X = c(1.20, 1.21, 3.06, 7.89, 5.67, 6.10, 3.90)))
summary(our_sample)
mu_sample <- extract(our_sample,
pars = 'mu')$`mu`
hist(mu_sample)
# ==== 2.2 A BIZARRE PRIOR DENSITY ==== #
our_data <- list(n = 20,
p = c(0.25057883,0.16862872,0.11989827,0.28149519,0.20427907,
0.16859187,0.40253736,0.09341611,0.14762340,0.14047014,
0.29998209,0.19349593,0.21179227,0.24900885,0.32570937,
0.12341203,0.20488021,0.33726469,0.08214418,0.41775598))
# Looking at the prior density
vals = rep(1:10)
vals
?rep
# Looking at the prior density
vals = seq(1, 10, 0.1)
vals
n = length(vals)
out = c()
out = rep(0, n)
for (i in 1:n) {
x = vals[i]
out[i] = x^3*exp(-x)*(sin(x)+1.2)
}
out
hist(out)
rstan:::rstudio_stanc("prac3_2.stan")
rstan:::rstudio_stanc("prac3_2.stan")
rstan:::rstudio_stanc("prac3_2.stan")
model2 = stan_model('prac3_2.stan')
sample = sampling(model2, data=our_data)
theta_sample = extract(sample, pars='theta')$`theta`
hist(theta_sample)
hist(out) # inverse shape of normal??
# Looking at the prior density...
vals = seq(1, 10, 0.1)
n = length(vals)
out = rep(0, n)
for (i in 1:n) {
x = vals[i]
# out[i] = x^3*exp(-x)*(sin(x)+1.2)
out[i] = 3*log(x) - x + log(sin(x) + 1.2)
}
hist(out) # inverse shape of normal??
model2 = stan_model('prac3_2.stan')
sample = sampling(model2, data=our_data)
theta_sample = extract(sample, pars='theta')$`theta`
hist(theta_sample)
hist(out) # inverse shape of normal??
for (i in 1:n) {
x = vals[i]
out[i] = x^3*exp(-x)*(sin(x)+1.2)
#out[i] = 3*log(x) - x + log(sin(x) + 1.2)
}
hist(out)
plot(vals, out, type='l')
hist(theta_sample)
plot(vals, out, type='l')
hist(theta_sample)
rstan:::rstudio_stanc("prac3_3.stan")
hie_data <- list(N = 40,
NK = 2,
NJ = c(2,2),
group = c(rep(1,30),
rep(2,10)),
subgroup = c(sort(rep(1:2,15)),
sort(rep(1:2,5))),
X = c(3,2,2,2,1,
2,3,4,0,0,
2,5,4,4,1,
3,3,3,4,4,
2,5,3,4,4,
1,1,3,3,5,
1,0,1,1,0,
1,0,1,2,0))
model3 = stan_model('prac3_3.stan')
sample = sampling(model3, data=hie_data)
lamda_sample = extract(sample, pars=c('group', 'subgroup'))
# ==== PART 3 - NN CLASSIFICATION USING THE IRIS DATASET
data(iris)
iris
names(iris)
dim(iris)
y<- class.ind( c(rep("s", 50), rep("c", 50), rep("v", 50)) )
?class.in
?class.ind
??class.ind
library(nnet)
y<- class.ind( c(rep("s", 50), rep("c", 50), rep("v", 50)) )
x <-iris[,-c(4,5)]
ind <- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
y_train <- y[ind,]
x_train <- x[ind,]
y_valid <- y[-ind,]
x_valid <- x[-ind,]
# PART 1
compound_dist = function(N, alpha) {
return(beta(alpha+1, N+1) / beta(alpha, 1))
}
a = compound_dist(2, 0.435)
b = compound_dist(1, 0.435)
c = compound_dist(0, 0.435)
a+b+c
a = compound_dist(2, 0.434)
b = compound_dist(1, 0.434)
c = compound_dist(0, 0.434)
a+b+c
a = compound_dist(2, 0.435)
b = compound_dist(1, 0.435)
c = compound_dist(0, 0.435)
a = compound_dist(2, 0.4345)
b = compound_dist(1, 0.4345)
c = compound_dist(0, 0.4345)
a+b+c
a = compound_dist(2, 0.4347)
b = compound_dist(1, 0.4347)
c = compound_dist(0, 0.4347)
a+b+c
a = compound_dist(2, 0.4348)
b = compound_dist(1, 0.4348)
c = compound_dist(0, 0.4348)
a+b+c
a = compound_dist(2, 0.4349)
b = compound_dist(1, 0.4349)
c = compound_dist(0, 0.4349)
a+b+c
