hie_data <- list(N = 40,
NK = 2,
NJ = c(2,2),
group = c(rep(1,30),
rep(2,10)),
subgroup = c(sort(rep(1:2,15)),
sort(rep(1:2,5))),
X = c(3,2,2,2,1,
2,3,4,0,0,
2,5,4,4,1,
3,3,3,4,4,
2,5,3,4,4,
1,1,3,3,5,
1,0,1,1,0,
1,0,1,2,0))
model3 = stan_model('prac3_3.stan')
library(rstan)
setwd('C:/Users/guyro/OneDrive/Y3/bayesian-modelling/practicals')
hie_data <- list(N = 40,
NK = 2,
NJ = c(2,2),
group = c(rep(1,30),
rep(2,10)),
subgroup = c(sort(rep(1:2,15)),
sort(rep(1:2,5))),
X = c(3,2,2,2,1,
2,3,4,0,0,
2,5,4,4,1,
3,3,3,4,4,
2,5,3,4,4,
1,1,3,3,5,
1,0,1,1,0,
1,0,1,2,0))
model3 = stan_model('prac3_3.stan')
# sample from the posterior
sample = sampling(model3, data=hie_data)
dotchart(hie_data$X, col = hie_data$group, pch = hie_data$subgroup)
samples = extract(sample)
names(samples)
alpha = samples$alpha
beta = samples$beta
hist(alpha)
hist(beta)
hist(1/alpha)
# extract the samples
extracted = extract(sample, pars=c("alpha", "beta[1]", "beta[2]"))
# sample from the posterior
sample = sampling(model3, data=hie_data, iter=10000)
# extract the samples
extracted = extract(sample, pars=c("alpha", "beta[1]", "beta[2]"))
plot(density(extracted$alpha))
plot(density(extracted$beta[1]))
plot(density(extracted$beta))
plot(density(extracted$`beta[1]`))
plot(density(extracted$`beta[2]`))
prior_var_alpha <- 2*1/((2+1+1)*(2+1)^2)
post_var_alpha <- var(extracted$alpha)
res_alpha = 1 - post_var_alpha/prior_var_alpha
prior_var_alpha
plot(density(extracted$alpha))
post_var_alpha
res_alpha
install.packages("e1071")
library(e1071)
set.seed(1)
x <- matrix(rnorm(20 * 2), ncol = 2)
y <- c(rep(-1, 10), rep(1, 10))
x[y == 1, ] <- x[y == 1, ] + 1
plot(x, col = (3 - y))
dat <- data.frame(x = x, y = as.factor(y))
dat
?svm
dat$x
dat$y
dat$x.1
svmfit = svm(x=c(dat$x.1, dat$x.2), y=dat$y, cost=10, kernel='linear', scale=FALSE)
svmfit
plot(svmfit, dat)
svmfit = svm(y~., data=dat, cost=10, kernel='linear', scale=FALSE)
svmfit
plot(svmfit, dat)
svmfit$index
summary(svmfit)
summary(svmfit)
svmfit = svm(y~., data=dat, cost=0.1, kernel='linear', scale=FALSE)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
svmfit = svm(y~., data=dat, cost=0.1, kernel='linear', scale=FALSE)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
?tune
tune(data=dat)
tune(y~., data=dat)
tune(METHOD=svm, y~., data=dat)
# Tuning the model (automatically) using cross-validation
# tune(METHOD=svm, y~., data=dat)
set.seed(1) # keep me for consistency reasons
#
tune.out <- tune(METHOD=svm,
y ~ ., data = dat,
kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100))
)
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)
xtest <- matrix(rnorm(20 * 2), ncol = 2)
ytest <- sample(c(-1, 1), 20, rep = TRUE)
xtest[ytest == 1, ] <- xtest[ytest == 1, ] + 1
testdat <- data.frame(x = xtest, y = as.factor(ytest))
predict(bestmod, data=testdat)
table(preds, dat$y)
preds = predict(bestmod, data=testdat)
table(preds, dat$y)
preds = predict(bestmod, data=testdat)
table(preds, dat$y)
# Tuning the model (automatically) using cross-validation
# tune(METHOD=svm, y~., data=dat)
set.seed(1) # keep me for consistency reasons
#
tune.out <- tune(METHOD=svm,
y ~ ., data = dat,
kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100))
)
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)
xtest <- matrix(rnorm(20 * 2), ncol = 2)
ytest <- sample(c(-1, 1), 20, rep = TRUE)
xtest[ytest == 1, ] <- xtest[ytest == 1, ] + 1
testdat <- data.frame(x = xtest, y = as.factor(ytest))
preds = predict(bestmod, data=testdat)
table(preds, dat$y)
# Now test the SVM with cost=0.01
svmfit2 = svm(y~., data=dat, cost=0.01, kernel='linear', scale=FALSE)
predictions = predict(svmfit2, data=testdat)
table(predictions, testdat$y)
table(preds, testdat$y)
table(predictions, testdat$y)
## Linearly separable case
x[y == 1, ] <- x[y == 1, ] + 0.5
plot(x, col = (y + 5) / 2, pch = 19)
dat <- data.frame(x = x, y = as.factor(y))
svmfit3 = svm(y~., data=dat, cost=1e5, kernel='linear', scale=FALSE)
dat <- data.frame(x = x, y = as.factor(y))
svmfit = svm(y~., data=dat, cost=0.1, kernel='linear', scale=FALSE)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
library(e1071)
set.seed(1)
x <- matrix(rnorm(20 * 2), ncol = 2)
y <- c(rep(-1, 10), rep(1, 10))
x[y == 1, ] <- x[y == 1, ] + 1
plot(x, col = (3 - y))
dat <- data.frame(x = x, y = as.factor(y))
svmfit = svm(y~., data=dat, cost=0.1, kernel='linear', scale=FALSE)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
svmfit = svm(y~., data=dat, cost=10, kernel='linear', scale=FALSE)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
svmfit = svm(y~., data=dat, cost=10, kernel='linear', scale=FALSE)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
set.seed(1)
x <- matrix(rnorm(20 * 2), ncol = 2)
y <- c(rep(-1, 10), rep(1, 10))
x[y == 1, ] <- x[y == 1, ] + 1
plot(x, col = (3 - y))
dat <- data.frame(x = x, y = as.factor(y))
svmfit = svm(y~., data=dat, cost=10, kernel='linear', scale=FALSE)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
summary(svmfit)
ytest <- sample(c(-1, 1), 20, rep = TRUE)
xtest[ytest == 1, ] <- xtest[ytest == 1, ] + 1
testdat <- data.frame(x = xtest, y = as.factor(ytest))
preds = predict(bestmod, data=testdat)
table(preds, testdat$y)
# Now test the SVM with cost=0.01
svmfit2 = svm(y~., data=dat, cost=0.01, kernel='linear', scale=FALSE)
predictions = predict(svmfit2, data=testdat)
table(predictions, testdat$y)
## Linearly separable case
x[y == 1, ] <- x[y == 1, ] + 0.5
plot(x, col = (y + 5) / 2, pch = 19)
dat <- data.frame(x = x, y = as.factor(y))
svmfit3 = svm(y~., data=dat, cost=1e5, kernel='linear', scale=FALSE)
plot(svmfit3, dat)
svmfit3 = svm(y~., data=dat, cost=1, kernel='linear', scale=FALSE)
plot(svmfit3, dat)
# Setup the data
set.seed(1)
N = 1000
xx = rnorm(N)
yy = 4 * xx^2 + 1 + rnorm(N)
class = sample(N, N/2)
yy[class] = yy[class] + 6
yy[-class] = yy[-class] - 6
x <- cbind(xx,yy)
plot(x[class,1], x[class,2], col = "red", xlab = "X", ylab = "Y", ylim = c(-6, 30)) +
points(x[-class,1], x[-class,2], col = "blue")
y = rep(-1, N)
y[class] = 1
data = data.frame(x = x, y = as.factor(y))
train = sample(N, N/2)
data_train = data[train, ]
data_eval = data[-train, ]
# This is why using linear prediction rule (kernel) will not work!
svm_linear = svm(y ~ ., data = data_train, kernel = "linear", cost = 1)
plot(svm_linear, data_train)
svm_poly = svm(
y ~ .,
data = data_train,
kernel = 'polynomial',
degree = 2,
gamma = 2,
coef0 = 0,
cost = 1
)
plot(svm_poly, data_train)
svm_poly = svm(
y ~ .,
data = data_train,
kernel = 'polynomial',
degree = 2,
gamma = 2,
coef0 = 0,
cost = 1,
scale=FALSE
)
plot(svm_poly, data_train)
?svm
tune.out = tune(METHOD=svm, y ~ ., data = data_train, kernel="polynomial", ranges = range_params)
# Now using cross-validation
range_params = list(cost = c(0.01, 0.05, 0.1), gamma = c(0.5,1,2.0), coef0 = c(0,1,2), degree = c(1,2,3))
tune.out = tune(METHOD=svm, y ~ ., data = data_train, kernel="polynomial", ranges = range_params)
summary(tune.out)
tune.out$best.model
tune.out$best.model$cost
library(rstan)
setwd('C:/Users/guyro/OneDrive/Y3/bayesian-modelling/practicals/prac4')
rstan:::rstudio_stanc("model1.stan")
rstan:::rstudio_stanc("model1.stan")
our_model <- stan_model('trunc.stan')
our_model <- stan_model('model1.stan')
our_sample <- sampling(our_model,
data = list(x=c(2.3,1.0,0.4,1.1,0.6)))
our_sample <- sampling(our_model,
data = list(x=c(2.3,1.0,0.4,1.1,0.6)))
our_sample <- sampling(our_model,
data = list(n=5, x=c(2.3,1.0,0.4,1.1,0.6)))
# print summary
summary(our_sample)$summary
# extract mean from sample
mu_sample = extract(our_sample, pars='mu')$mu
plot(our_sample)
traceplot(our_sample)
our_sample <- sampling(our_model,
data = list(n=5, x=c(2.3,1.0,0.4,1.1,0.6)), iter=10000)
# print summary
summary(our_sample)$summary
# extract mean from sample
mu_sample = extract(our_sample, pars='mu')$mu
plot(our_sample)
traceplot(our_sample)
## LOGISTIC REGRESSION ====
x <- matrix(c(1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,
1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,
1,2,3,1,2,3,1,2,3,1,2,3,1,2,3),nrow = 15,ncol = 3)
colnames(x) <- c('x_1','x_2','x_3')
y <- c(0,0,0,1,0,0,0,1,1,1,0,1,0,1,1)
# plot the data (explanatory)
pairs(x)
# add the response to plot
pairs(cbind(x, y))
# add the response to plot
pairs(cbind(x, y))
rstan:::rstudio_stanc("model2.stan")
our_sample = sampling(our_model, data = list(N=15, x=x, y=y))
# Approximate the posterior mode
our_model = stan_model('model2.stan')
our_sample = sampling(our_model, data = list(N=15, x=x, y=y))
summary(our_sample)
beta_means = extract(our_sample, pars=c('beta[1]', 'beta[2]', 'beta[3]'))
beta_means
beta_means$`beta[1]`
mean(beta_means$`beta[1]`)
mean(beta_means)
apply(betas, mean)
apply(betas, function(beta) {
return(mean(beta`))
})
betas = extract(our_sample, pars=c('beta[1]', 'beta[2]', 'beta[3]'))
apply(betas, function(beta) {
return(mean(beta`))
mean(betas[1])
betas = extract(our_sample, pars=c('beta[1]', 'beta[2]', 'beta[3]'))
mean(betas[1])
mean(betas$`beta[1]`)
mean(betas$`beta[2]`)
mean(betas$`beta[3]`)
hist(betas$`beta[1]`)
hist(betas$`beta[2]`)
hist(betas$`beta[3]`)
## CENSORED DATA =====
rain_data <- data.frame(year = sort(rep(2016:2022,3)),
sunday_number = rep(1:3,7),
rainfall = c( 99.78906,174.15984,144.26242,
156.17152,174.89812,228.20058,
144.66969,201.70483,254.25715,
252.56507,168.98985,156.84697,
169.79339,227.39160,253.90171,
88.87051,200.98870,232.28434,
129.17346,137.15685,204.28012))
rstan:::rstudio_stanc("model3.stan")
our_model = stan_model('model3.stan')
our_sample = sampling(our_model, data=list(N = 21, R = rain_data$rainfall))
summary(our_sample)
rstan:::rstudio_stanc("model3.stan")
